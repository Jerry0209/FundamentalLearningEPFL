{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 3: Unsupervised Learning\n",
    "\n",
    "# Part 1: Classification with K-means algorithm\n",
    "\n",
    "The K-means algorithm is a fundamental tool among the unsupervised learning model. Consider a problem with a dataset $\\mathcal{D} = \\left\\{x_i\\right\\}_{i=1}^n$ where $x_i \\in \\mathbb{R}^d$ with no labels, we are aiming at finding some hidden structure within the data, namely, we would like to find clusters in the dataset. Classifiers have been studied in TP but mainly for supervised learning, here the data are not labeled. \n",
    "\n",
    "The K-means algorithm tries to classify the dataset in $K$ clusters. Each cluster is represented by a centroid, meaning the average of the points within the cluster. We note $C_k$ the set of points of a cluster $k$ and $\\mu_k$ its centroid. Then, the algorithm minimizes the intra-cluster variance, in other words, it tries to reduce the distance between the points of the cluster and the centroid. \n",
    "\n",
    "More technically, the algorithm works iteratively in two main steps: \n",
    " - Points are assigned to clusters based on their proximity to existing centroid\n",
    " - Centroids are updated by taking the average of the points assigned to each cluster.\n",
    "\n",
    "\n",
    "\n",
    "The Loss function of the problem can be written as:\n",
    "$$J(\\mu_1, ..., \\mu_K) = \\sum_{i=1}^{n} \\, \\lVert x_i - \\mu(i)\\rVert^2 \\; ,$$\n",
    "where $\\mu(i)$ is the centroid of the cluster assignated to $x_i$. \n",
    "\n",
    "We want to check the understanding of the choice of this loss function. Does it match the aforementioned rules? Then, could it help to understand if the algorithm converges?\n",
    "\n",
    "Additionnally, until Question 8, we assume that the clusters $C_k$ are disjoint, especially at initialization. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Secret is in the Notation $\\mu(i)$\n",
    "The confusion usually comes from the term $\\mu(i)$.\n",
    "* $\\mu_k$ (with a subscript $k$) refers to a specific centroid (e.g., Centroid #1, Centroid #2).\n",
    "* $\\mu(i)$ (function of $i$) is a shorthand way of saying: **\"The centroid that point $x_i$ currently belongs to.\"**\n",
    "\n",
    "#### A Concrete Example\n",
    "Imagine you have **3 Data Points** ($x_1, x_2, x_3$) and **2 Clusters** (Centroids $\\mu_1, \\mu_2$).\n",
    "\n",
    "* **$x_1$** belongs to Cluster 1.\n",
    "* **$x_2$** belongs to Cluster 1.\n",
    "* **$x_3$** belongs to Cluster 2.\n",
    "\n",
    "The formula $J = \\sum_{i=1}^{3} \\lVert x_i - \\mu(i)\\rVert^2$ expands like this:\n",
    "\n",
    "1.  **For $i=1$:** $x_1$ is in Cluster 1, so $\\mu(1)$ is $\\mu_1$. Term: $\\lVert x_1 - \\mu_1 \\rVert^2$\n",
    "2.  **For $i=2$:** $x_2$ is in Cluster 1, so $\\mu(2)$ is $\\mu_1$. Term: $\\lVert x_2 - \\mu_1 \\rVert^2$\n",
    "3.  **For $i=3$:** $x_3$ is in Cluster 2, so $\\mu(3)$ is $\\mu_2$. Term: $\\lVert x_3 - \\mu_2 \\rVert^2$\n",
    "\n",
    "**Total J:**\n",
    "$$J = \\lVert x_1 - \\mu_1 \\rVert^2 + \\lVert x_2 - \\mu_1 \\rVert^2 + \\lVert x_3 - \\mu_2 \\rVert^2$$\n",
    "\n",
    "### Summary\n",
    "* The sum is over $i$ because we calculate the error **point-by-point**.\n",
    "* $\\mu(i)$ dynamically grabs the correct $\\mu_k$ for that specific point.\n",
    "\n",
    "**Next Step:** This concept is exactly what **Question 1** asks you to prove. Would you like to see how to transform this \"point-wise\" sum ($\\sum_i$) into the \"cluster-wise\" sum ($\\sum_k$)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 1:\n",
    "\n",
    "First, let us familiarize ourselves with the loss function:\n",
    "\n",
    " - Prove the second equality:\n",
    "\n",
    "   $$J(\\mu_1, ..., \\mu_K) = \\sum_{i=1}^{n} \\, \\lVert x_i - \\mu(i)\\rVert^2 = \\sum_{k=1}^{K} \\sum_{x_i \\in C_k} \\, \\lVert x_i - \\mu_k \\rVert^2$$\n",
    "\n",
    " - What does the term $\\sum_{x_i \\in C_k} \\, \\lVert x_i - \\mu_k \\rVert^2$ represent?\n",
    " \n",
    " - Explain why this form of the loss function is more convenient.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Q1 Ans**\n",
    "\n",
    "To prove the equality, we first define the dataset as $D = \\{x_i\\}_{i=1}^n$. We assume that we classify the dataset into $K$ disjoint clusters, which means that the union of all clusters $C_1 \\cup C_2 \\dots \\cup C_K$ is equal to the whole dataset $D$, and every point $x_i$ belongs to exactly one specific cluster. If a point $x_i$ is in cluster $C_k$, then its assigned centroid $\\mu(i)$ is simply $\\mu_k$.\n",
    "\n",
    "We can rewrite the total loss function by grouping the points based on which cluster they belong to. Instead of summing $i$ from 1 to $n$ in one go, we can split the sum into $K$ parts, calculating the error for each cluster separately:\n",
    "\n",
    "$$J(\\mu_1, ..., \\mu_K) = \\sum_{x_i \\in C_1} \\lVert x_i - \\mu_1 \\rVert^2 + \\sum_{x_i \\in C_2} \\lVert x_i - \\mu_2 \\rVert^2 + \\dots + \\sum_{x_i \\in C_K} \\lVert x_i - \\mu_K \\rVert^2$$\n",
    "\n",
    "Since this summation covers every point in the dataset exactly once, we can combine these terms using the summation over $k$. This proves that the two forms are mathematically identical:\n",
    "\n",
    "$$\\sum_{i=1}^{n} \\, \\lVert x_i - \\mu(i)\\rVert^2 = \\sum_{k=1}^{K} \\sum_{x_i \\in C_k} \\, \\lVert x_i - \\mu_k \\rVert^2$$\n",
    "\n",
    "For the second question, the term $\\sum_{x_i \\in C_k} \\, \\lVert x_i - \\mu_k \\rVert^2$ represent the sum of the square distance of each points in cluster $k$ to its centroid. Physically, this measures the intra-cluster variance. It tells us how \"spread out\" the points are within that specific group. A lower value means the cluster is very compact and the points are gathered tightly around the center $\\mu_k$.\n",
    "\n",
    "Finally, this form of the loss function is more convenient because it separates the problem into independent parts. When we want to minimize the loss to find the optimal position for a centroid $\\mu_k$, we can ignore all other clusters. The terms for other clusters $C_{j}$ (where $j \\neq k$) do not depend on $\\mu_k$, so they act as constants. This means we can simplify the optimization problem significantly:\n",
    "\n",
    "$$\\frac{\\partial J}{\\partial \\mu_k} = \\frac{\\partial}{\\partial \\mu_k} \\left( \\sum_{j=1}^K \\sum_{x_i \\in C_j} \\lVert x_i - \\mu_j \\rVert^2 \\right) = \\frac{\\partial}{\\partial \\mu_k} \\left( \\sum_{x_i \\in C_k} \\lVert x_i - \\mu_k \\rVert^2 \\right)$$\n",
    "\n",
    "This independence allow us to update each centroid separately without worrying about the rest of the dataset at the same time.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 2:\n",
    "\n",
    "Let us focus on the first point of the algorihtm, let us consider a single point $x_i$ and add the time dependency. Also, we denote by a $'$ the variables after the new assignement of the data points. \n",
    "\n",
    "So that, the variables are denoted by: $(\\cdot)^t \\to (\\cdot)^t\\,{}' \\to (\\cdot)^{t+1} \\to (\\cdot)^{t+1}\\,{}' \\to (\\cdot)^{t+2}$\n",
    "\n",
    "\n",
    "Thus, at each step time $t$, the new assignements of the variables leads to: (no proof required)\n",
    "\n",
    "$$ \\mu^t(i)' = {\\rm argmin}_{\\mu \\in \\left\\{\\mu^t_k\\right\\}_k} \\lVert x_i - \\mu \\rVert^2$$\n",
    "\n",
    "For instance, at time $t$ before new assignements, the vector $x_i$ belongs to a cluster $k$, while after the next assignement, it now belongs to the cluster $k'$ (it can be the same or different from the cluster $k$). \n",
    "\n",
    "\n",
    "Thus, for the whole dataset, the algorithm updates the assignement as: $\\left\\{\\mu^t(i) \\right\\} \\to \\left\\{\\mu^t(i)'\\right\\}$.\n",
    "\n",
    "\n",
    "Compare $\\lVert x_i - \\mu^t(i) \\rVert^2$ and $\\lVert x_i - \\mu^t(i)' \\rVert^2$ for a given point. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Q2 Ans**\n",
    "\n",
    "To compare the two terms, we first look at the definition of the new assignment provided in the question. The algorithm updates the assignment for a single point $x_i$ by choosing the closest centroid from the current set of centroids:\n",
    "\n",
    "$$\\mu^t(i)' = {\\rm argmin}_{\\mu \\in \\left\\{\\mu^t_k\\right\\}_k} \\lVert x_i - \\mu \\rVert^2$$\n",
    "\n",
    "This expression tells us that $\\mu^t(i)'$ is the specific centroid that achieves the minimum possible distance to $x_i$ among all available clusters at time $t$. On the other hand, $\\mu^t(i)$ represents the centroid that was assigned to the point in the previous step. It is important to note that the old centroid $\\mu^t(i)$ belongs to the set of all existing centroids $\\left\\{\\mu^t_k\\right\\}_k$, which means it was one of the candidates we considered during the minimization process.\n",
    "\n",
    "Because $\\mu^t(i)'$ is selected specifically to be the minimum, the squared distance to it must be less than or equal to the squared distance to any other candidate in the set, including the old assignment $\\mu^t(i)$. Therefore, we can conclude the following inequality:\n",
    "\n",
    "$$\\lVert x_i - \\mu^t(i)' \\rVert^2 \\leq \\lVert x_i - \\mu^t(i) \\rVert^2$$\n",
    "\n",
    "This inequality implies that for every single data point, the reassignment step strictly reduces the distance to the cluster center, or keeps it the same if the point is already at the optimal cluster. The error for a specific point never increases during this step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 3:\n",
    "\n",
    "We recall that we denote by a $'$ the variables after the new assignement of the data points, so that: $\\mu^t(i) \\to \\mu^t(i)'$ and $J_t \\to J_t'$\n",
    "\n",
    "Thanks to the previous question, compare $J_t$ and ${J_t}'$. \n",
    "\n",
    "Hint: Pick the right formula between the two given for $J$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Q3 Ans**\n",
    "To compare $J_t$ and $J_t'$, we should use the first formula of the loss function which sums over the index $i$. This is the most convenient choice because in the previous question, we analyzed the behavior of individual data points. We can write the expressions for the loss before and after the assignment as:\n",
    "\n",
    "$$J_t(\\mu_1, \\dots, \\mu_K) = \\sum_{i=1}^{n} \\lVert x_i - \\mu^t(i)\\rVert^2$$\n",
    "\n",
    "$$J_t'(\\mu_1, \\dots, \\mu_K) = \\sum_{i=1}^{n} \\lVert x_i - \\mu^t(i)'\\rVert^2$$\n",
    "\n",
    "From Question 2, we know that for each data point $x_i$, the squared distance to the new centroid is less than or equal to the squared distance to the old centroid. This is because we specifically chose the closest centroid during the update. So for every $i$, we have the inequality:\n",
    "\n",
    "$$\\lVert x_i - \\mu^t(i)' \\rVert^2 \\leq \\lVert x_i - \\mu^t(i) \\rVert^2$$\n",
    "\n",
    "Since this inequality hold for every single point in the dataset, we can sum it up from $i=1$ to $n$. If every term on the left side is smaller or equal to the corresponding term on the right side, the total sum must also be smaller or equal:\n",
    "\n",
    "$$\\sum_{i=1}^{n} \\lVert x_i - \\mu^t(i)' \\rVert^2 \\leq \\sum_{i=1}^{n} \\lVert x_i - \\mu^t(i) \\rVert^2$$\n",
    "\n",
    "Replacing the sums with the notation for $J$, we can conclude:\n",
    "\n",
    "$$J_t'(\\mu_1, \\dots, \\mu_K) \\leq J_t(\\mu_1, \\dots, \\mu_K)$$\n",
    "\n",
    "This proves that the assignment step of the algorithm always decrease the total loss function (or keeps it the same), ensuring the algorithm improves the solution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 4:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we can go ahead by studying the second point of the algorithm. It wants to minimize the intra-cluster variance:\n",
    "\n",
    "$$ \\left\\{\\mu^{t+1}_k\\right\\}_k = {\\rm argmin}_{\\left\\{\\mu_k\\right\\}_k \\in \\mathbb{R}^d} J'_t\\left( \\left\\{\\mu_k\\right\\}_k \\right) = {\\rm argmin}_{\\left\\{\\mu_k\\right\\}_k \\in \\mathbb{R}^d} \\sum_{k=1}^{K} \\sum_{x_i \\in C^t_k{}'} \\, \\lVert x_i - \\mu_k \\rVert^2 $$\n",
    "\n",
    "\n",
    "Show that the optimization can be done cluster-wise:\n",
    "\n",
    "$$\\mu^{t+1}_k = {\\rm argmin}_{\\mu_k \\in \\mathbb{R}^d}  \\sum_{x_i \\in C^t_k{}'} \\, \\lVert x_i - \\mu_k \\rVert^2 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Q4 Ans**\n",
    "\n",
    "To show that the optimization can be done cluster-wise, we first expand the objective function $J'_t$ to see its components clearly. The objective is defined as a sum over all clusters $k$ from 1 to $K$:\n",
    "\n",
    "$$J'_t\\left( \\left\\{\\mu_k\\right\\}_k \\right) = \\sum_{k=1}^{K} \\sum_{x_i \\in C^t_k{}'} \\lVert x_i - \\mu_k \\rVert^2$$\n",
    "\n",
    "We can write this sum explicitly as a series of independent terms:\n",
    "\n",
    "$$J'_t = \\left( \\sum_{x_i \\in C^t_1{}'} \\lVert x_i - \\mu_1 \\rVert^2 \\right) + \\dots + \\left( \\sum_{x_i \\in C^t_k{}'} \\lVert x_i - \\mu_k \\rVert^2 \\right) + \\dots + \\left( \\sum_{x_i \\in C^t_K{}'} \\lVert x_i - \\mu_K \\rVert^2 \\right)$$\n",
    "\n",
    "From the problem description, we know that the clusters $C_k$ are disjoint and the variables $\\mu_1, \\dots, \\mu_K$ are independent. This means that the term for cluster $k$ depends *only* on the variable $\\mu_k$ and contains no other centroids. Changing $\\mu_k$ has absolutely no effect on the other terms in the sum.\n",
    "\n",
    "Mathematically, when we want to minimize a sum of independent functions $F(x, y) = f(x) + g(y)$, it is equivalent to minimizing each function separately. Therefore, we can push the \"argmin\" operator inside the sum and solve for each $\\mu_k$ individually:\n",
    "\n",
    "$${\\rm argmin}_{\\left\\{\\mu_k\\right\\}_k} J'_t = {\\rm argmin}_{\\mu_1} \\left( \\dots \\right) + \\dots + {\\rm argmin}_{\\mu_k} \\left( \\sum_{x_i \\in C^t_k{}'} \\lVert x_i - \\mu_k \\rVert^2 \\right)$$\n",
    "\n",
    "Thus, the update rule for the specific centroid $k$ becomes:\n",
    "\n",
    "$$\\mu^{t+1}_k = {\\rm argmin}_{\\mu_k \\in \\mathbb{R}^d}  \\sum_{x_i \\in C^t_k{}'} \\, \\lVert x_i - \\mu_k \\rVert^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "#### Question 5\n",
    "\n",
    "Show that the new centroids of time $t+1$ are computed according to the following equality:\n",
    "\n",
    "$$ \\mu_k^{t+1} = \\frac{1}{|C^t_k{}'|}\\sum_{x_i \\in C^t_k{}'} x_i $$\n",
    "\n",
    "Does it correspond to what you expected from the algorithm? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Q5 Ans**\n",
    "\n",
    "To find the optimal centroid $\\mu_k^{t+1}$, we need to minimize the intra-cluster variance. As noted in the handwriting, the objective function for a specific cluster $k$ is a convex quadratic function. The minimum of such a function is obtained where its gradient with respect to $\\mu_k$ is equal to zero.\n",
    "\n",
    "First, let us expand the squared Euclidean norm using vector algebra. We recall that $\\lVert v \\rVert^2 = v^T v$. So for the objective function, we can write:\n",
    "\n",
    "$$J(\\mu_k) = \\sum_{x_i \\in C^t_k{}'} \\lVert x_i - \\mu_k \\rVert^2 = \\sum_{x_i \\in C^t_k{}'} (x_i - \\mu_k)^T (x_i - \\mu_k)$$\n",
    "\n",
    "$$= \\sum_{x_i \\in C^t_k{}'} \\left( x_i^T x_i - 2x_i^T \\mu_k + \\mu_k^T \\mu_k \\right)$$\n",
    "\n",
    "Now we compute the gradient $\\nabla_{\\mu_k}$ of this expression. We treat $x_i$ as a constant. The derivative of $x_i^T x_i$ is 0. The derivative of the linear term $-2x_i^T \\mu_k$ is $-2x_i$, and the derivative of the quadratic term $\\mu_k^T \\mu_k$ is $2\\mu_k$. We can apply the gradient operator inside the summation:\n",
    "\n",
    "$$\\nabla_{\\mu_k} J(\\mu_k) = \\sum_{x_i \\in C^t_k{}'} \\left( 0 - 2x_i + 2\\mu_k \\right) = \\sum_{x_i \\in C^t_k{}'} (-2x_i + 2\\mu_k)$$\n",
    "\n",
    "To find the minimum, we set this gradient to zero. We can factor out the constant 2 and split the summation into two distinct parts:\n",
    "\n",
    "$$-2 \\sum_{x_i \\in C^t_k{}'} x_i + 2 \\sum_{x_i \\in C^t_k{}'} \\mu_k = 0$$\n",
    "\n",
    "$$\\sum_{x_i \\in C^t_k{}'} x_i = \\sum_{x_i \\in C^t_k{}'} \\mu_k$$\n",
    "\n",
    "In the term on the right, $\\mu_k$ does not depend on the index $i$. Therefore, we are simply summing the vector $\\mu_k$ repeatedly. The number of times we sum it is equal to the number of points in the cluster. As indicated in the notes, we denote $|C^t_k{}'|$ as the cardinality of the set, which counts how many items are inside $C^t_k{}'$. This allows us to write:\n",
    "\n",
    "$$\\sum_{x_i \\in C^t_k{}'} x_i = |C^t_k{}'| \\cdot \\mu_k$$\n",
    "\n",
    "$$\\mu_k^{t+1} = \\frac{1}{|C^t_k{}'|} \\sum_{x_i \\in C^t_k{}'} x_i$$\n",
    "\n",
    "Yes, this result corresponds exactly to our expectations. The formula represents the standard mathematical definition of the mean (average). It confirms that to minimize the squared error within a cluster, the representative centroid should be placed at the exact arithmetic average of all the points assigned to that cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 6:\n",
    "\n",
    "If we focus on a cluster $k$ at time $t$ after the assignement, noted $C^t_k {}'$, could you compare $\\sum_{x_i \\in C^t_k{}'} \\, \\lVert x_i - \\mu_k^t \\rVert^2$ and $\\sum_{x_i \\in C^t_k{}'} \\, \\lVert x_i - \\mu_k^{t+1} \\rVert^2$ ?\n",
    "\n",
    "What can you say about ${J_t}'$ and $J_{t+1}$ ? Hint: Use the right formula between the two given for $J$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Q6 Ans**\n",
    "\n",
    "To compare the two sums, we look back at the definition of the new centroid from Question 4. We defined $\\mu_k^{t+1}$ as the specific vector that minimizes the sum of squared distances for the points in the new cluster $C^t_k{}'$. Mathematically, this means it is the global minimum of the function. Because it is the minimum, plugging any other vector into this sumâ€”including the old centroid $\\mu_k^t$â€”must result in a value that is larger or equal. Therefore, for every cluster $k$, we have the following inequality:\n",
    "\n",
    "$$\\sum_{x_i \\in C^t_k{}'} \\lVert x_i - \\mu_k^{t+1} \\rVert^2 \\leq \\sum_{x_i \\in C^t_k{}'} \\lVert x_i - \\mu_k^t \\rVert^2$$\n",
    "\n",
    "Now we can analyze $J_{t'}$ and $J_{t+1}$ using the cluster-wise formula for the loss function. $J_{t'}$ represents the total loss when the points are in their new clusters $C^t_k{}'$ but the centroids are still at their old positions $\\mu_k^t$. On the other hand, $J_{t+1}$ is the total loss after we update the centroids to $\\mu_k^{t+1}$. We can express them as sums over $k$:\n",
    "\n",
    "$$J_{t'} = \\sum_{k=1}^{K} \\sum_{x_i \\in C^t_k{}'} \\lVert x_i - \\mu_k^t \\rVert^2$$\n",
    "\n",
    "$$J_{t+1} = \\sum_{k=1}^{K} \\sum_{x_i \\in C^t_k{}'} \\lVert x_i - \\mu_k^{t+1} \\rVert^2$$\n",
    "\n",
    "Since we know that the inequality holds for each individual cluster term in the sum, it implies that the sum of all terms must also satisfy the inequality. If every part of a sum gets smaller or stays the same, the total result must also be smaller. Thus, by summing our first inequality from $k=1$ to $K$, we derive the final relation:\n",
    "\n",
    "$$\\sum_{k=1}^{K} \\sum_{x_i \\in C^t_k{}'} \\lVert x_i - \\mu_k^{t+1} \\rVert^2 \\leq \\sum_{k=1}^{K} \\sum_{x_i \\in C^t_k{}'} \\lVert x_i - \\mu_k^t \\rVert^2$$\n",
    "\n",
    "$$J_{t+1} \\leq J_{t'}$$\n",
    "\n",
    "This confirms that the second step of the algorithm, the update of centroids, always decrease the total loss function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 7:\n",
    "\n",
    "Putting together the Questions 3 and 5, compare $J_t$ and $J_{t+1}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Q7 Ans**\n",
    "To compare $J_t$ and $J_{t+1}$, we simply need to chain together the results we proved in the previous questions. The K-means algorithm consists of two phases in one full iteration: the assignment phase and the update phase. We can analyze the change in the loss function for each phase separately to see the total effect.\n",
    "\n",
    "First, from Question 3, we analyzed the assignment step where we fix the centroids and move data points to their closest cluster. We proved that this operation never increase the total distance because we explicitly choose the minimum distance for every point. Therefore, the loss function after reassignment $J_t'$ is less than or equal to the initial loss $J_t$:\n",
    "\n",
    "$$J_t' \\leq J_t$$\n",
    "\n",
    "Second, from Question 5 and Question 6, we looked at the update step where we fix the assignments and calculate new centroids. We showed that taking the average of the points is the optimal way to minimize the intra-cluster variance. Because the new centroid $\\mu^{t+1}$ is the mathematical minimum, it gives a lower or equal error compared to the old centroid. This give us the second inequality:\n",
    "\n",
    "$$J_{t+1} \\leq J_t'$$\n",
    "\n",
    "Now, we can combine these two inequalities into a single chain. Since $J_{t+1}$ is smaller than $J_t'$, and $J_t'$ is smaller than $J_t$, it follows by transitivity that:\n",
    "\n",
    "$$J_{t+1} \\leq J_t' \\leq J_t$$\n",
    "\n",
    "This result implies that the total error $J_{t+1}$ is always less than or equal to $J_t$. This proves the monotonicity of the K-means algorithm, guaranteeing that the objective function will decrease or stay the same after every complete loop, and the model will not diverge."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 8:\n",
    "\n",
    "After recalling a trivial lower bound for the sequence $(J_t)_{t \\geq 0}$, what can you say about the convergence?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Q8 Ans**\n",
    "To analyze the convergence, we first identify a trivial lower bound for the loss function. The function $J$ is defined as the sum of squared Euclidean norms:\n",
    "\n",
    "$$J(\\mu_1, ..., \\mu_K) = \\sum_{i=1}^{n} \\lVert x_i - \\mu(i)\\rVert^2$$\n",
    "\n",
    "Since the Euclidean norm $\\lVert \\cdot \\rVert$ represents a distance, it is always non-negative. Furthermore, the square of a real number is always non-negative. Therefore, we are summing $n$ non-negative terms, which implies that the total sum cannot be negative. Thus, the sequence of loss values $(J_t)_{t \\geq 0}$ is bounded from below by 0:\n",
    "\n",
    "$$J_t \\geq 0 \\quad \\text{for all } t$$\n",
    "\n",
    "Next, we recall the result from Question 7, where we proved that the algorithm minimizes the objective function at each step. The assignment step minimizes $J$ with respect to the cluster assignments $c$, and the update step minimizes $J$ with respect to the centroids $\\mu$. This process is known as coordinate descent. Because each step reduces (or maintains) the error, the sequence is monotonically decreasing:\n",
    "\n",
    "$$J_{t+1} \\leq J_t$$\n",
    "\n",
    "Finally, we can apply the Monotone Convergence Theorem from real analysis. This theorem states that any sequence of real numbers that is monotonically decreasing and bounded from below must converge to a limit. Since our sequence $(J_t)$ is always decreasing but can never go below 0, it must converge to a finite value $J^*$.\n",
    "\n",
    "$$\\lim_{t \\to \\infty} J_t = J^*$$\n",
    "\n",
    "This proves that the K-means algorithm will always stabilize in terms of the loss function. While it is theoretically possible for the assignments to oscillate between states with the exact same energy, the value of the loss function itself is guaranteed to converge."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "#### Question 9:\n",
    "\n",
    "We just proved that the algorithm converges, but what about its stability:\n",
    "\n",
    "Let us suppose that the data are sampled from a mixture of $K$ Gaussian, where the choice of $K$ is free for this question. Do you imagine a situation where the algorithm does not classify the data at all? Please design and explain the situation as clearly as possible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Q9 Ans**\n",
    "\n",
    "While we have proved that the algorithm converges, it does not guarantee finding the best possible solution. This is because the distortion function $J$ is a non-convex function. The K-means algorithm performs coordinate descent on $J$, which means it steps down the \"valley\" of the loss function, but it can easily get stuck in a local optimum instead of the global minimum. To illustrate a situation where the algorithm fails to classify the data correctly, we can design a scenario based on symmetry.\n",
    "\n",
    "Let us suppose we have a dataset sampled from a mixture of $K=2$ Gaussian clusters that are well-separated along the x-axis. We can define the true centers of these clusters at:\n",
    "\n",
    "$$\\text{True Center } A = (-10, 0)$$\n",
    "$$\\text{True Center } B = (10, 0)$$\n",
    "\n",
    "Ideally, the algorithm should classify the data into a \"Left Cluster\" and a \"Right Cluster.\" However, if we choose the initial centroids poorly, the algorithm might fail. Let us initialize the two centroids on the vertical axis, which acts as the perpendicular bisector between the two data clouds:\n",
    "\n",
    "$$\\mu_1 = (0, 5)$$\n",
    "$$\\mu_2 = (0, -5)$$\n",
    "\n",
    "In the assignment step, the algorithm looks for the closest centroid for each point. For any data point located in the top half of the plane (positive y), the distance to the top centroid $\\mu_1$ is smaller than the distance to the bottom centroid $\\mu_2$, regardless of whether the point belongs to the left or right cluster. Consequently, $\\mu_1$ captures the top half of Cluster A and the top half of Cluster B. Similarly, $\\mu_2$ captures the bottom halves of both clusters.\n",
    "\n",
    "In the update step, the new centroids are calculated as the average of their assigned points. Because the data is symmetric around the y-axis, the x-coordinates of the left and right points cancel each other out during the averaging process.\n",
    "\n",
    "$$\\mu_{1, x}^{new} \\approx \\frac{-10 + 10}{2} = 0$$\n",
    "\n",
    "This means the centroids will move up and down along the y-axis but will never move left or right to find the true clusters. The algorithm will converge to a stable state where it separates the data into \"Top\" and \"Bottom\" groups, which is completely orthogonal to the true \"Left\" and \"Right\" structure. This situation confirms that K-means is susceptible to local optima and that the initialization is crucial for its success."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 10:\n",
    "\n",
    "What can you say about those configurations of centroids? What does it imply concerning the minima? Conclude your arguments by discussing the convexity of the problem. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Q10 Ans**\n",
    "\n",
    "The configurations of centroids we saw in Question 9 imply that the K-means algorithm is sensitive to initialization. Depending on where we start, the algorithm can converge to different final states. This implies that the objective function $J$ does not have a single, unique valley that leads to the best solution. Instead, the landscape of the loss function is rugged, containing many different valleys of varying depths known as local minima. While the coordinate descent approach guarantees that the loss will decrease and converge, it does not guarantee that the valley we fall into is the deepest one, which would be the global minimum.\n",
    "\n",
    "This existence of multiple distinct stable states allows us to conclude that the K-means optimization problem is non-convex. In mathematical optimization, a strictly convex function resembles a smooth bowl where any ball dropped inside will roll to the exact same bottom point. Because K-means allows for suboptimal stable states where the algorithm gets stuck, the function cannot be convex. The non-convexity arises largely because of the \"min\" operator in the assignment step; when a centroid moves just far enough, a data point suddenly switches its assignment from one cluster to another, creating \"kinks\" or breaks in the function's curvature.\n",
    "\n",
    "To prove formally that $J$ is not convex, we can use the definition of convexity. A function is convex if, for any two points in the solution space, the function value at the midpoint is lower than or equal to the average of the function values at the two endpoints. If we find one case where the midpoint has a higher error, the function is not convex. Let us consider a simple 1D dataset with two points $x_1 = -1$ and $x_2 = 1$, and we want to find $K=2$ centroids.\n",
    "\n",
    "First, consider Configuration A, which is an optimal solution:\n",
    "$$\\mu_A = \\{-1, 1\\}$$\n",
    "$$J(\\mu_A) = (-1 - (-1))^2 + (1 - 1)^2 = 0$$\n",
    "\n",
    "Next, consider Configuration B, which is the same centroids but swapped (also optimal):\n",
    "$$\\mu_B = \\{1, -1\\}$$\n",
    "$$J(\\mu_B) = (-1 - (-1))^2 + (1 - 1)^2 = 0$$\n",
    "(Note: In Configuration B, $x_1$ assigns to the second centroid and $x_2$ assigns to the first, so the distance is still zero).\n",
    "\n",
    "Now, let us calculate the loss at the geometric midpoint of these two configurations:\n",
    "$$\\mu_{mid} = \\frac{\\mu_A + \\mu_B}{2} = \\left\\{ \\frac{-1+1}{2}, \\frac{1+(-1)}{2} \\right\\} = \\{0, 0\\}$$\n",
    "\n",
    "In this midpoint configuration, both centroids are at 0. Both data points $x_1$ and $x_2$ are distance 1 away from the origin:\n",
    "$$J(\\mu_{mid}) = (-1 - 0)^2 + (1 - 0)^2 = 1 + 1 = 2$$\n",
    "\n",
    "Finally, we compare the midpoint loss to the average loss of the endpoints:\n",
    "$$J(\\mu_{mid}) = 2$$\n",
    "$$\\text{Average}(J(\\mu_A), J(\\mu_B)) = \\frac{0 + 0}{2} = 0$$\n",
    "\n",
    "Since $2 > 0$, the inequality required for convexity is violated. The \"bowl\" curves upward in the middle, proving the function is non-convex. Because of this property, a common practical solution is to run K-means multiple times with different random initializations and pick the result with the lowest distortion $J$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 11:\n",
    "\n",
    "We can also quickly generalize our algorithm.\n",
    "\n",
    "In some situation, you are aiming at favoring some directions in your data and penalizing the others, so that you can weigh the euclidean distance according to:\n",
    "\n",
    "$$d^{(w)}(x_{i}, \\mu(i)) = \\frac{\\sum_{j=1}^d w_i(x_{ij} - \\mu(i)_j)^2}{\\sum_{j=1}^d w_j} $$\n",
    "\n",
    "Show that with a change of variables, the problem remains the same.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Q11 Ans**\n",
    "\n",
    "To solve Question 11, we need to show that introducing directional weights into the distance metric is mathematically equivalent to solving the standard K-means problem on a rescaled version of the dataset.\n",
    "\n",
    "First, let us address the notation in the problem statement. The formula provided in the question uses $w_i$ inside a sum over dimensions $j$, but the denominator sums $w_j$, and the context mentions \"favoring some directions.\" This strongly suggests a typo where the weight should depend on the dimension $j$, not the data point index $i$. We will assume the correct term is $w_j$.\n",
    "\n",
    "We start by writing down the total loss function using the weighted distance provided. Let $W_{total} = \\sum_{j=1}^d w_j$ be the normalization constant. The weighted objective function $J_{weighted}$ sums this distance over all points $i=1$ to $n$:\n",
    "\n",
    "$$J_{weighted} = \\sum_{i=1}^n d^{(w)}(x_{i}, \\mu(i)) = \\sum_{i=1}^n \\frac{1}{W_{total}} \\sum_{j=1}^d w_j (x_{ij} - \\mu(i)_j)^2$$\n",
    "\n",
    "Since $W_{total}$ is just a constant positive scalar, minimizing $J_{weighted}$ is equivalent to minimizing the numerator alone. Let us focus on the term inside the summation:\n",
    "\n",
    "$$J_{numerator} = \\sum_{i=1}^n \\sum_{j=1}^d w_j (x_{ij} - \\mu(i)_j)^2$$\n",
    "\n",
    "We want to transform this equation into the standard squared Euclidean form $\\sum (a - b)^2$. We can do this by absorbing the weight $w_j$ into the squared term. Recall that $c \\cdot z^2 = (\\sqrt{c} \\cdot z)^2$ for any non-negative weight $c$.\n",
    "\n",
    "We define a change of variables to create a new dataset $\\tilde{x}$ and new centroids $\\tilde{\\mu}$:\n",
    "\n",
    "$$\\tilde{x}_{ij} = \\sqrt{w_j} \\cdot x_{ij}$$\n",
    "\n",
    "$$\\tilde{\\mu}(i)_j = \\sqrt{w_j} \\cdot \\mu(i)_j$$\n",
    "\n",
    "This transformation effectively stretches or shrinks the data along each dimension $j$ based on how important that direction is (the square root of the weight $w_j$). Now, we substitute these new variables back into our objective function:\n",
    "\n",
    "$$J_{numerator} = \\sum_{i=1}^n \\sum_{j=1}^d \\left( \\sqrt{w_j} (x_{ij} - \\mu(i)_j) \\right)^2$$\n",
    "\n",
    "$$= \\sum_{i=1}^n \\sum_{j=1}^d \\left( \\sqrt{w_j} x_{ij} - \\sqrt{w_j} \\mu(i)_j \\right)^2$$\n",
    "\n",
    "$$= \\sum_{i=1}^n \\sum_{j=1}^d \\left( \\tilde{x}_{ij} - \\tilde{\\mu}(i)_j \\right)^2$$\n",
    "\n",
    "We can recognize that the inner sum $\\sum_{j=1}^d (\\dots)^2$ is simply the standard squared Euclidean norm for the transformed vectors. Therefore, the weighted problem can be rewritten as:\n",
    "\n",
    "$$J_{weighted} \\propto \\sum_{i=1}^n \\lVert \\tilde{x}_i - \\tilde{\\mu}(i) \\rVert^2$$\n",
    "\n",
    "This result shows that the objective function is exactly the same as the standard K-means loss function, but applied to the transformed data $\\tilde{x}$. The problem remains the same structurally. Practically, this means you do not need to implement a special \"weighted K-means\" algorithm. You can simply multiply each column of your data matrix by $\\sqrt{w_j}$, run the standard K-means algorithm, and then divide the resulting centroids by $\\sqrt{w_j}$ to get the solution in the original space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Restricted Boltzmann Machine\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "\n",
    "The Boltzmann Machine have been inspired by thermodynamic and statistical physics models, more precisely they are part of the Energy Models using the well known Boltzmann Distribution as written in physics style:\n",
    "\n",
    "$$ P\\left( E \\right)  = \\frac{1}{Z} \\exp \\left( -\\frac{E}{k_b T} \\right)$$\n",
    "\n",
    "It becomes in statistical inference framework:\n",
    "$$\n",
    "P(\\mathbf{v} | J, \\mathbf{b}) \\propto e^{\\mathbf{v}^TJ\\mathbf{v} + \\mathbf{b}^T\\mathbf{v}} = e^{-E(\\mathbf{v})}\n",
    "$$\n",
    "where:\n",
    "- $\\mathbf{v}\\in\\mathbb{R}^n:$ The binary vector with components $v_i = 0 \\; {\\rm or} \\; 1$\n",
    "\n",
    "- $J \\in \\mathbb{R}^{n \\times n}:$ The coupling matrix\n",
    "\n",
    "- $\\mathbf{b} \\in  \\mathbb{R}^n$: Field\n",
    "\n",
    "- $E(\\mathbf{v}) \\in  \\mathbb{R}$: Energy\n",
    "\n",
    "\n",
    "However, one problem arised with initial Boltzmann Machine (BM) -- like its parent models in statistical physics (as the SK model) -- all the units are interacting through complicated dependencies. For example, if we consider 3 components of $\\mathbf{v}$: $v_1$, $v_2$, and $v_3$, there are trivial interactions such as one modelised by $P(v_1, v_2)$ corresponding to the correlation between the two first components of $\\mathbf{v}$, but there are also none trivial interactions. Indeed, if some term like $P(v_1, v_2 | v_3)$ which suggests that the correlation $x_1$ and $v_2$ depends on $v_3$ and this is clearly none linear.\n",
    "\n",
    "A really ingenious way to overcome this situation is to replace all the tricky interactions between the units $\\mathbf{v}\\in\\mathbb{R}^n$ by connections through hidden units $\\mathbf{h}\\in\\mathbb{R}^m$, artifically created. Indeed, correlations between two units $v_1$ and $v_2$ (specially the dependency of their correlations on other units $v_3$, $v_4$,...) can be atrificially replaced by introducing a third unit $h_1$ and considerin only linear correlations between $v_1 \\leftrightarrow h_1$, $h_1 \\leftrightarrow v_2$ and $v_1 \\leftrightarrow v_2$. The units $v_i$ are now called the visible units. This model is the most known version of BMs. \n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"boltzmannmachine.png\" alt=\"Diagram here\" />\n",
    "</div>\n",
    "\n",
    "\n",
    "However, this model is still fully connected and makes the computation really costful. Then, one can even simplify the model by considering zero intra layer interractions. This simplified model is call Restricted Boltzmann Machine (RBM) (Physics Nobel Price 2024 ğŸ¥³).\n",
    "\n",
    "Thus, the RBM architecture consists of two layers of binary stochastic units: a $\\textbf{visible layer}$ $\\mathbf{v}$ and a $\\textbf{hidden layer}$ $\\mathbf{h}$. The layers are fully connected, but there are no connections within a layer, making the model a $\\textbf{bipartite graph}$. \n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"rbm.png\" alt=\"Diagram here\" />\n",
    "</div>\n",
    "\n",
    "Restricted Boltzmann Machines (RBMs) are a class of energy-based probabilistic graphical models that are commonly used in machine learning for tasks such as dimensionality reduction, feature learning, and generative modeling.\n",
    "\n",
    "### Energy Function and Probabilities\n",
    "\n",
    "The joint configuration of the visible units $\\mathbf{v} \\in \\{0, 1\\}^d$ and the hidden units $\\mathbf{h} \\in \\{0, 1\\}^m$ is associated with an $\\textbf{energy function}$, defined as:\n",
    "\n",
    "$$ E(\\mathbf{v}, \\mathbf{h}) = -\\mathbf{v}^\\top \\mathbf{W} \\mathbf{h} - \\mathbf{b}^\\top \\mathbf{v} - \\mathbf{c}^\\top \\mathbf{h}$$\n",
    "where:\n",
    "- $\\mathbf{W} \\in \\mathbb{R}^{d \\times m}$ is the weight matrix connecting the visible and hidden units,\n",
    "- $\\mathbf{b} \\in \\mathbb{R}^d$ field of the visible units or also called the biases of the visible units,\n",
    "- $\\mathbf{c} \\in \\mathbb{R}^m$ field of the hidden units of also called the biases of the hidden units.\n",
    "\n",
    "The energy function determines the joint probability distribution over $\\mathbf{v}$ and $\\mathbf{h}$:\n",
    "$$ P(\\mathbf{v}, \\mathbf{h}) = \\frac{1}{Z} \\exp(-E(\\mathbf{v}, \\mathbf{h})) $$\n",
    "where $Z$ is the $\\textbf{partition function}$, ensuring normalization:\n",
    "\n",
    "$$ Z = \\sum_{\\mathbf{v}, \\mathbf{h}} \\exp(-E(\\mathbf{v}, \\mathbf{h})) $$\n",
    "\n",
    "\n",
    "The marginal probability of the visible units $\\mathbf{v}$ is obtained by summing over all possible configurations of the hidden units:\n",
    "\n",
    "$$ P(\\mathbf{v}) = \\frac{1}{Z} \\sum_{\\mathbf{h}} \\exp(-E(\\mathbf{v}, \\mathbf{h})). $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the material you provided, here is an explanation of the **origin** of the Boltzmann Machine and the specific **problems** it was designed to solve.\n",
    "\n",
    "æ ¹æ®ä½ æä¾›çš„ææ–™ï¼Œä»¥ä¸‹æ˜¯å¯¹ç»å°”å…¹æ›¼æœºå™¨çš„**èµ·æº**åŠå…¶æ—¨åœ¨è§£å†³çš„å…·ä½“**é—®é¢˜**çš„è§£é‡Šã€‚\n",
    "\n",
    "---\n",
    "\n",
    "### 1. Where does it come from? (Origin)\n",
    "### 1. å®ƒä»å“ªé‡Œæ¥ï¼Ÿï¼ˆèµ·æºï¼‰\n",
    "\n",
    "**Physics Inspiration**\n",
    "**ç‰©ç†å­¦çµæ„Ÿ**\n",
    "\n",
    "* **English:** According to the text, the Boltzmann Machine is directly inspired by **thermodynamic and statistical physics models**. It belongs to a class called \"Energy Models.\"\n",
    "* **Chinese:** æ ¹æ®æ–‡æœ¬ï¼Œç»å°”å…¹æ›¼æœºç›´æ¥å—**çƒ­åŠ›å­¦å’Œç»Ÿè®¡ç‰©ç†æ¨¡å‹**çš„å¯å‘ã€‚å®ƒå±äºâ€œèƒ½é‡æ¨¡å‹â€è¿™ä¸€ç±»ã€‚\n",
    "\n",
    "* **English:** Its mathematical foundation comes from the **Boltzmann Distribution** used in physics to describe the probability of a state based on its energy:\n",
    "    $$P(E) = \\frac{1}{Z} \\exp\\left(-\\frac{E}{k_b T}\\right)$$\n",
    "    The machine learning version adapts this formula to calculate the probability of data vectors ($\\mathbf{v}$) based on an Energy function ($E$).\n",
    "* **Chinese:** å®ƒçš„æ•°å­¦åŸºç¡€æ¥è‡ªç‰©ç†å­¦ä¸­ä½¿ç”¨çš„**ç»å°”å…¹æ›¼åˆ†å¸ƒ**ï¼Œç”¨äºæ ¹æ®èƒ½é‡æè¿°æŸç§çŠ¶æ€çš„æ¦‚ç‡ã€‚æœºå™¨å­¦ä¹ ç‰ˆæœ¬é‡‡ç”¨äº†è¿™ä¸ªå…¬å¼ï¼Œæ ¹æ®èƒ½é‡å‡½æ•° ($E$) æ¥è®¡ç®—æ•°æ®å‘é‡ ($\\mathbf{v}$) çš„æ¦‚ç‡ã€‚\n",
    "\n",
    "---\n",
    "\n",
    "### 2. What problem does it solve?\n",
    "### 2. å®ƒè§£å†³äº†ä»€ä¹ˆé—®é¢˜ï¼Ÿ\n",
    "\n",
    "The material describes a two-step evolution to solve two specific problems: **Complexity** and **Computational Cost**.\n",
    "ææ–™æè¿°äº†ä¸€ä¸ªä¸¤æ­¥æ¼”è¿›è¿‡ç¨‹ï¼Œæ—¨åœ¨è§£å†³ä¸¤ä¸ªå…·ä½“é—®é¢˜ï¼š**å¤æ‚æ€§**å’Œ**è®¡ç®—æˆæœ¬**ã€‚\n",
    "\n",
    "#### Problem A: Modeling Complex Dependencies\n",
    "#### é—®é¢˜ Aï¼šå¯¹å¤æ‚çš„ä¾èµ–å…³ç³»å»ºæ¨¡\n",
    "\n",
    "* **The Issue (Original Concept):** In early models, visible units had complicated, non-linear interactions (e.g., the correlation between $v_1$ and $v_2$ might depend on $v_3$). This is hard to model directly.\n",
    "* **é—®é¢˜ï¼ˆåŸå§‹æ¦‚å¿µï¼‰ï¼š** åœ¨æ—©æœŸçš„æ¨¡å‹ä¸­ï¼Œå¯è§å•å…ƒä¹‹é—´å­˜åœ¨å¤æ‚çš„éçº¿æ€§ç›¸äº’ä½œç”¨ï¼ˆä¾‹å¦‚ï¼Œ$v_1$ å’Œ $v_2$ ä¹‹é—´çš„ç›¸å…³æ€§å¯èƒ½å–å†³äº $v_3$ï¼‰ã€‚è¿™å¾ˆéš¾ç›´æ¥å»ºæ¨¡ã€‚\n",
    "\n",
    "* **The Solution (Hidden Units):** The material describes an \"ingenious way\" to fix this: introducing **Hidden Nodes** ($\\mathbf{h}$). Instead of $v_1$ interacting directly with $v_2$ in complex ways, they interact through a hidden unit ($h_1$). This allows the model to capture complex patterns using simpler linear connections.\n",
    "* **è§£å†³æ–¹æ¡ˆï¼ˆéšè—å•å…ƒï¼‰ï¼š** ææ–™æè¿°äº†ä¸€ä¸ªâ€œå·§å¦™çš„æ–¹æ³•â€æ¥è§£å†³è¿™ä¸ªé—®é¢˜ï¼šå¼•å…¥**éšè—èŠ‚ç‚¹** ($\\mathbf{h}$)ã€‚$v_1$ ä¸å†ç›´æ¥ä»¥å¤æ‚çš„æ–¹å¼ä¸ $v_2$ äº¤äº’ï¼Œè€Œæ˜¯é€šè¿‡éšè—å•å…ƒ ($h_1$) è¿›è¡Œäº¤äº’ã€‚è¿™ä½¿å¾—æ¨¡å‹èƒ½å¤Ÿä½¿ç”¨æ›´ç®€å•çš„çº¿æ€§è¿æ¥æ¥æ•æ‰å¤æ‚çš„æ¨¡å¼ã€‚\n",
    "\n",
    "#### Problem B: Computational Cost (The birth of RBM)\n",
    "#### é—®é¢˜ Bï¼šè®¡ç®—æˆæœ¬ï¼ˆRBM çš„è¯ç”Ÿï¼‰\n",
    "\n",
    "* **The Issue (Standard Boltzmann Machine):** Even with hidden units, the standard Boltzmann Machine is **fully connected** (every node connects to every other node). The text states this \"makes the computation really costful.\"\n",
    "* **é—®é¢˜ï¼ˆæ ‡å‡†ç»å°”å…¹æ›¼æœºï¼‰ï¼š** å³ä½¿æœ‰äº†éšè—å•å…ƒï¼Œæ ‡å‡†çš„ç»å°”å…¹æ›¼æœºä¹Ÿæ˜¯**å…¨è¿æ¥çš„**ï¼ˆæ¯ä¸ªèŠ‚ç‚¹éƒ½ä¸æ‰€æœ‰å…¶ä»–èŠ‚ç‚¹ç›¸è¿ï¼‰ã€‚æ–‡æœ¬æŒ‡å‡ºè¿™â€œä½¿å¾—è®¡ç®—æˆæœ¬éå¸¸é«˜æ˜‚â€ã€‚\n",
    "\n",
    "* **The Solution (Restricted Boltzmann Machine - RBM):** To solve this, the connections are **restricted**.\n",
    "    * There are **zero intra-layer interactions** (no connections between visible nodes, no connections between hidden nodes).\n",
    "    * This creates a **Bipartite Graph** structure (as seen in your second image).\n",
    "* **è§£å†³æ–¹æ¡ˆï¼ˆå—é™ç»å°”å…¹æ›¼æœº - RBMï¼‰ï¼š** ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œè¿æ¥è¢«**é™åˆ¶**äº†ã€‚\n",
    "    * **å±‚å†…äº¤äº’ä¸ºé›¶**ï¼ˆå¯è§èŠ‚ç‚¹ä¹‹é—´æ²¡æœ‰è¿æ¥ï¼Œéšè—èŠ‚ç‚¹ä¹‹é—´ä¹Ÿæ²¡æœ‰è¿æ¥ï¼‰ã€‚\n",
    "    * è¿™è¿™å°±äº§ç”Ÿäº†**äºŒåˆ†å›¾**ç»“æ„ï¼ˆå¦‚ä½ çš„ç¬¬äºŒå¼ å›¾æ‰€ç¤ºï¼‰ã€‚\n",
    "\n",
    "### Summary\n",
    "### æ€»ç»“\n",
    "\n",
    "* **Origin:** Physics (Thermodynamics).\n",
    "* **Problem:** Modeling complex data relationships was too hard, and the calculations were too slow.\n",
    "* **Solution:** Use **Hidden Units** to simplify the relationships, and use the **Restricted** architecture (no layer-internal connections) to make the math faster and feasible.\n",
    "\n",
    "* **èµ·æºï¼š** ç‰©ç†å­¦ï¼ˆçƒ­åŠ›å­¦ï¼‰ã€‚\n",
    "* **é—®é¢˜ï¼š** å¯¹å¤æ‚çš„æ•°æ®å…³ç³»å»ºæ¨¡å¤ªéš¾ï¼Œä¸”è®¡ç®—é€Ÿåº¦å¤ªæ…¢ã€‚\n",
    "* **è§£å†³æ–¹æ¡ˆï¼š** ä½¿ç”¨**éšè—å•å…ƒ**æ¥ç®€åŒ–å…³ç³»ï¼Œå¹¶ä½¿ç”¨**å—é™**æ¶æ„ï¼ˆæ— å±‚å†…è¿æ¥ï¼‰ä½¿æ•°å­¦è®¡ç®—æ›´å¿«ã€æ›´å¯è¡Œã€‚\n",
    "\n",
    "---\n",
    "\n",
    "**Next Step:**\n",
    "The text mentions that RBMs create a **\"Bipartite Graph\"** (äºŒåˆ†å›¾). Would you like me to explain why this specific structure makes the math so much easier compared to the full Boltzmann Machine?\n",
    "æ–‡æœ¬ä¸­æåˆ° RBM å»ºç«‹äº†ä¸€ä¸ª**â€œäºŒåˆ†å›¾â€**ã€‚ä½ æƒ³è®©æˆ‘è§£é‡Šä¸€ä¸‹ï¼Œä¸ºä»€ä¹ˆè¿™ç§ç‰¹å®šçš„ç»“æ„æ¯”å…¨è¿æ¥çš„ç»å°”å…¹æ›¼æœºèƒ½è®©æ•°å­¦è®¡ç®—å˜å¾—å¦‚æ­¤ç®€å•å—ï¼Ÿ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "è¿™æ˜¯å—é™ç»å°”å…¹æ›¼æœºï¼ˆRBMï¼‰ç†è®ºä¸­æœ€æ ¸å¿ƒçš„å…¬å¼ä¹‹ä¸€ã€‚å®ƒæè¿°çš„æ˜¯ï¼š**å¦‚æœæˆ‘ä»¬å¿½ç•¥éšè—å±‚å‘ç”Ÿçš„ä¸€åˆ‡ï¼Œä»…ä»…è§‚å¯Ÿåˆ°æŸä¸€ä¸ªç‰¹å®šæ•°æ®æ ·æœ¬ï¼ˆæ¯”å¦‚ä¸€å¼ å›¾ç‰‡ $\\mathbf{v}$ï¼‰çš„æ¦‚ç‡æ˜¯å¤šå°‘ã€‚**\n",
    "\n",
    "æˆ‘ä»¬å¯ä»¥ä»ä¸‰ä¸ªå±‚é¢æ¥ç†è§£è¿™ä¸ªå…¬å¼ï¼š**ç›´è§‰å«ä¹‰**ã€**æ•°å­¦æ“ä½œ**å’Œ**å®é™…ä½œç”¨**ã€‚\n",
    "\n",
    "### 1. ç›´è§‰å«ä¹‰ï¼šæ‰€æœ‰å¯èƒ½æ€§çš„â€œæ€»å’Œâ€\n",
    "### Intuitive Meaning: The \"Sum\" of All Possibilities\n",
    "\n",
    "* **Joint vs. Marginal (è”åˆ vs. è¾¹ç¼˜):**\n",
    "    * $P(\\mathbf{v}, \\mathbf{h})$ æ˜¯è”åˆæ¦‚ç‡ã€‚å®ƒæè¿°çš„æ˜¯ï¼šâ€œçœ‹è§å›¾ç‰‡ $\\mathbf{v}$ **ä¸”** éšè—ç‰¹å¾æ˜¯ $\\mathbf{h}$â€ çš„æ¦‚ç‡ã€‚\n",
    "    * $P(\\mathbf{v})$ æ˜¯è¾¹ç¼˜æ¦‚ç‡ã€‚å®ƒæè¿°çš„æ˜¯ï¼šâ€œçœ‹è§å›¾ç‰‡ $\\mathbf{v}$â€ çš„æ¦‚ç‡ï¼Œ**ä¸ç®¡ $\\mathbf{h}$ æ˜¯ä»€ä¹ˆ**ã€‚\n",
    "\n",
    "* **Analogy (æ¯”å–»):**\n",
    "    æƒ³è±¡ $\\mathbf{v}$ æ˜¯ä½ åœ¨æ£®æ—é‡Œçœ‹åˆ°çš„ä¸€ä¸ªâ€œè„šå°â€ã€‚\n",
    "    $\\mathbf{h}$ æ˜¯å¯èƒ½ç•™ä¸‹è¿™ä¸ªè„šå°çš„åŠ¨ç‰©ï¼ˆç†Šã€è€è™ã€æˆ–è€…ç©¿ç€å¤§é‹å­çš„äººï¼‰ã€‚\n",
    "    \n",
    "    è¦è®¡ç®—**è¿™ä¸ªè„šå°å‡ºç°çš„æ¦‚ç‡ $P(\\mathbf{v})$**ï¼Œä½ ä¸èƒ½åªè€ƒè™‘â€œç†Šâ€ç•™ä¸‹çš„æ¦‚ç‡ï¼Œä½ å¿…é¡»æŠŠâ€œç†Šç•™ä¸‹çš„æ¦‚ç‡â€ + â€œè€è™ç•™ä¸‹çš„æ¦‚ç‡â€ + â€œäººç•™ä¸‹çš„æ¦‚ç‡â€... **å…¨éƒ¨åŠ èµ·æ¥**ã€‚\n",
    "    \n",
    "    è¿™å°±æ˜¯å…¬å¼ä¸­ $\\sum_{\\mathbf{h}}$ çš„å«ä¹‰ï¼šæŠŠæ‰€æœ‰å¯èƒ½äº§ç”Ÿ $\\mathbf{v}$ çš„éšè—æƒ…å†µ $\\mathbf{h}$ çš„è´¡çŒ®å€¼å…¨éƒ¨åŠ æ€»ã€‚\n",
    "\n",
    "---\n",
    "\n",
    "### 2. æ•°å­¦æ“ä½œï¼šæ¶ˆé™¤æœªçŸ¥å˜é‡\n",
    "### Mathematical Operation: Removing the Unknowns\n",
    "\n",
    "è®©æˆ‘ä»¬æ‹†è§£å…¬å¼ä¸­çš„æ¯ä¸€éƒ¨åˆ†ï¼š\n",
    "\n",
    "$$P(\\mathbf{v}) = \\frac{1}{Z} \\sum_{\\mathbf{h}} \\exp(-E(\\mathbf{v}, \\mathbf{h}))$$\n",
    "\n",
    "1.  **$E(\\mathbf{v}, \\mathbf{h})$ (èƒ½é‡ Energy):**\n",
    "    è¿™æ˜¯ä¸€ä¸ªè¡¡é‡ $\\mathbf{v}$ å’Œ $\\mathbf{h}$ â€œå…¼å®¹æ€§â€çš„åˆ†æ•°ã€‚èƒ½é‡è¶Šä½ï¼Œå®ƒä»¬è¶ŠåŒ¹é…ã€‚\n",
    "    * *Lower Energy = Better Match.*\n",
    "\n",
    "2.  **$\\exp(-E(\\dots))$ (éå½’ä¸€åŒ–æ¦‚ç‡ Unnormalized Probability):**\n",
    "    è¿™æ˜¯ä¸€ä¸ªæ­£æ•°ã€‚èƒ½é‡è¶Šä½ï¼Œè¿™ä¸ªå€¼è¶Šå¤§ã€‚å®ƒä»£è¡¨æŸç§ç‰¹å®šçš„éšè—çŠ¶æ€ $\\mathbf{h}$ å¯¹å¯è§çŠ¶æ€ $\\mathbf{v}$ çš„â€œæ”¯æŒç¨‹åº¦â€ã€‚\n",
    "\n",
    "3.  **$\\sum_{\\mathbf{h}}$ (å¯¹ $\\mathbf{h}$ æ±‚å’Œ Sum over h):**\n",
    "    å› ä¸º $\\mathbf{h}$ æ˜¯éšè—çš„ï¼ˆæˆ‘ä»¬åœ¨ç°å®ä¸­çœ‹ä¸åˆ°å®ƒï¼‰ï¼Œå®ƒæ˜¯**ä¸ç¡®å®š**çš„ã€‚\n",
    "    ä¸ºäº†å¾—åˆ° $\\mathbf{v}$ çš„çœŸå®æ¦‚ç‡ï¼Œæˆ‘ä»¬å¿…é¡»éå† $\\mathbf{h}$ çš„æ‰€æœ‰å¯èƒ½å–å€¼ï¼ˆæ¯”å¦‚ $h=\\{0,0\\}, \\{0,1\\}, \\{1,0\\}, \\{1,1\\}$ï¼‰ï¼Œç®—å‡ºå®ƒä»¬æ¯ä¸€ä¸ªä¸ $\\mathbf{v}$ çš„åŒ¹é…åº¦ï¼Œç„¶ååŠ åœ¨ä¸€èµ·ã€‚\n",
    "    * è¿™ä¸ªè¿‡ç¨‹åœ¨æ•°å­¦ä¸Šå« **Marginalization (è¾¹ç¼˜åŒ–)**ã€‚\n",
    "\n",
    "4.  **$\\frac{1}{Z}$ (å½’ä¸€åŒ– Normalization):**\n",
    "    è¿™åªæ˜¯ä¸ºäº†ç¡®ä¿æœ€åç®—å‡ºæ¥çš„ç»“æœæ˜¯ä¸€ä¸ª 0 åˆ° 1 ä¹‹é—´çš„æ¦‚ç‡å€¼ã€‚\n",
    "\n",
    "---\n",
    "\n",
    "### 3. å®é™…ä½œç”¨ï¼šæ¨¡å‹è®­ç»ƒçš„ç›®æ ‡\n",
    "### Practical Use: The Goal of Training\n",
    "\n",
    "ä½ å¯èƒ½ä¼šé—®ï¼šâ€œä¸ºä»€ä¹ˆæˆ‘ä»¬è¦ç®—è¿™ä¸ªï¼Ÿâ€\n",
    "\n",
    "å› ä¸ºåœ¨è®­ç»ƒ RBM æ—¶ï¼Œæˆ‘ä»¬åªæœ‰æ•°æ® $\\mathbf{v}$ï¼ˆæ¯”å¦‚ä¸€å †æ‰‹å†™æ•°å­—çš„å›¾ç‰‡ï¼‰ï¼Œæ²¡æœ‰ $\\mathbf{h}$ã€‚\n",
    "æˆ‘ä»¬çš„ç›®æ ‡æ˜¯è°ƒæ•´æƒé‡ $\\mathbf{W}$ï¼Œä½¿å¾—æ¨¡å‹ç”Ÿæˆæˆ‘ä»¬è®­ç»ƒæ•°æ®çš„æ¦‚ç‡ $P(\\mathbf{v})$ **æœ€å¤§åŒ–**ã€‚\n",
    "\n",
    "* å¦‚æœæˆ‘ä»¬ç»™æ¨¡å‹çœ‹ä¸€å¼ çœŸå®çš„ç‹—çš„å›¾ç‰‡ $\\mathbf{v}_{dog}$ï¼Œæˆ‘ä»¬å¸Œæœ› $P(\\mathbf{v}_{dog})$ å¾ˆé«˜ã€‚\n",
    "* è¿™æ„å‘³ç€æˆ‘ä»¬éœ€è¦è°ƒæ•´å‚æ•°ï¼Œä½¿å¾—**æ€»èƒ½é‡**ï¼ˆåœ¨æ‰€æœ‰å¯èƒ½çš„ $\\mathbf{h}$ ç»„åˆä¸‹ï¼‰å°½å¯èƒ½ä½ã€‚\n",
    "\n",
    "### æ€»ç»“ (Summary)\n",
    "\n",
    "è¿™ä¸ªå…¬å¼æ˜¯åœ¨è¯´ï¼š\n",
    "> **â€œä¸ºäº†ç®—å‡ºè¿™ä¸ªæ•°æ® $\\mathbf{v}$ å‡ºç°çš„å¯èƒ½æ€§ï¼Œè¯·æŠŠæ‰€æœ‰èƒ½é€šè¿‡éšè—å±‚ $\\mathbf{h}$ è§£é‡Šå‡º $\\mathbf{v}$ çš„è·¯å¾„éƒ½åŠ èµ·æ¥ã€‚â€**\n",
    "\n",
    "---\n",
    "\n",
    "**ä¸‹ä¸€æ­¥ (Next Step):**\n",
    "è™½ç„¶è¿™ä¸ªæ±‚å’Œç¬¦å· $\\sum_{\\mathbf{h}}$ çœ‹èµ·æ¥å¾ˆå¯æ€•ï¼ˆå› ä¸º $\\mathbf{h}$ çš„ç»„åˆå¯èƒ½æœ‰æŒ‡æ•°çº§é‚£ä¹ˆå¤šï¼‰ï¼Œä½†åœ¨ RBM ä¸­ï¼Œç”±äºå±‚å†…æ²¡æœ‰è¿æ¥ï¼Œæˆ‘ä»¬å¯ä»¥ç”¨ä¸€ä¸ªéå¸¸æ¼‚äº®çš„æ•°å­¦æŠ€å·§æŠŠè¿™ä¸ªè¿åŠ å˜æˆç®€å•çš„è¿ä¹˜ï¼ˆè¿™å°±å¼•å‡ºäº† **Free Energy è‡ªç”±èƒ½** çš„æ¦‚å¿µï¼‰ã€‚\n",
    "\n",
    "**ä½ æƒ³çœ‹å®ƒæ˜¯å¦‚ä½•é€šè¿‡â€œè‡ªç”±èƒ½â€å…¬å¼è¢«ç®€åŒ–è®¡ç®—çš„å—ï¼Ÿ**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 12:\n",
    "\n",
    "Write a valid expression of the energy $E(\\textbf{v}, \\textbf{h})$ in the case of a BM (non-restricted) with an hidden layer. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Q12 Ans**\n",
    "\n",
    "To write the valid expression for the energy function in a general Boltzmann machine, we first need to decompose the total units x into two subsets: the visible units v and the latent or hidden units h. Unlike the Restricted Boltzmann Machine where layers are independent, the general Boltzmann machine allows connections between all variables, meaning we must account for interactions within the visible layer and within the hidden layer as well.\n",
    "\n",
    "Following the mathematical convention presented by Goodfellow et al. (2016, Eq. 20.3), the valid expression for the energy function E(v, h) is defined as:\n",
    "\n",
    "$$E(\\mathbf{v}, \\mathbf{h}) = -\\mathbf{v}^\\top \\mathbf{R} \\mathbf{v} - \\mathbf{v}^\\top \\mathbf{W} \\mathbf{h} - \\mathbf{h}^\\top \\mathbf{S} \\mathbf{h} - \\mathbf{b}^\\top \\mathbf{v} - \\mathbf{c}^\\top \\mathbf{h}$$\n",
    "\n",
    "In this expression, the matrix W represents the weights connecting the visible and hidden units, which is the standard interaction term. However, because this is a non-restricted machine, we also include the matrix R and matrix S. The matrix R captures the pairwise interactions between the visible units themselves, and S capture the interactions among the hidden units. Finally, the vectors b and c corresponds to the bias parameters for the visible and hidden layers respectively. This full connectivity allow the model to represent higher-order correlations that simpler models cannot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conditional Independence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 13:\n",
    "\n",
    "One of the key properties of RBMs is the $\\textbf{conditional independence}$ between units within a layer:\n",
    "\n",
    "Compute the conditional probability and show that:\n",
    "\n",
    "$$ P(h_j = 1 | \\mathbf{v}) = \\sigma\\left(c_j + \\sum_{i} v_i W_{ij}\\right) $$\n",
    "and\n",
    "$$ P(v_i = 1 | \\mathbf{h}) = \\sigma\\left(b_i + \\sum_{j} h_j W_{ij}\\right) $$\n",
    "\n",
    "where $\\sigma(x) = \\frac{1}{1 + e^{-x}}$ is the sigmoid activation function.\n",
    "\n",
    "This bipartite structure enables efficient Gibbs sampling for approximating the intractable joint distribution $P(\\mathbf{v}, \\mathbf{h})$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Q13 Ans**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning in RBMs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 14:\n",
    "\n",
    "Training an RBM involves maximizing the likelihood of the data distribution. To do so we are aiming at using a gradient descent/ascent on the weights (and biases).\n",
    "\n",
    "Compute the log-likelihood $\\mathcal{L}(\\mathbf{v})$, remember that the model is part of the unsupervised learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Q14 Ans**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 15:\n",
    "\n",
    "Compute the gradient of the log-likelihood with respect to the weights $\\mathbf{W}$ and the biases $\\mathbf{b}$, $\\mathbf{c}$ : "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Q15 Ans**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, it should be possible to implement the RBM!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 16: (Open question)\n",
    "\n",
    "While it seems possible to run RBM algorithm, note that the second term in the gradient w.r.t. $\\mathbf{W}$ is computationally expensive due to the intractability of $Z$, the approximation Contrastive Divergence - k is often use. Research what is this approximation, is this approximation enough, why? Explain it with your own words and cite the papers you used for your documentation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Q16 Ans**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applications of RBMs\n",
    "\n",
    "RBMs are widely used in tasks such as:\n",
    "\n",
    "- $\\textbf{Dimensionality reduction}$: Similar to PCA but capable of capturing non-linear structures,\n",
    "- $\\textbf{Feature learning}$: For pre-training deep neural networks,\n",
    "- $\\textbf{Collaborative filtering}$: Used in recommendation systems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reference\n",
    "\n",
    "[1] Goodfellow, I., Bengio, Y., & Courville, A. (2016). *Deep learning*. MIT Press."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
