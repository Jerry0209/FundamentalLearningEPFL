{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "71ae769f-affc-4fca-ade9-4f60cdafac4c",
   "metadata": {},
   "source": [
    "å¥½çš„ï¼Œæˆ‘å¾ˆä¹æ„ä¸ºä½ çš„å®¶åº­ä½œä¸šï¼ˆHomework 2ï¼‰ä¸­çš„æ¯ä¸€é“é¢˜æä¾›ä¸€äº›æç¤ºã€‚\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ¤– Hw2 æç¤º\n",
    "\n",
    "è¿™é‡Œæ˜¯é’ˆå¯¹ä½  .ipynb æ–‡ä»¶ä¸­æ¯ä¸€é“é¢˜çš„æç¤ºï¼š\n",
    "\n",
    "### Part 1: Expected Prediction Risk (EPR)\n",
    "\n",
    "**Q1: ç®€åŒ– EPR è¡¨è¾¾å¼ï¼Œè¯æ˜å…¶è¢«æ¡ä»¶å‡å€¼æœ€å°åŒ–ã€‚**\n",
    "* **æç¤º**ï¼šä½¿ç”¨å…¨æœŸæœ›å®šå¾‹ (Law of Total Expectation)ï¼Œå°† EPR å†™ä½œ $\\text{EPR}[f] = \\mathbb{E}_X[\\mathbb{E}_{Y|X}[(Y - f(X))^2 | X]]$ã€‚\n",
    "* **å…³é”®æ€è·¯**ï¼šå¯¹äºä¸€ä¸ªå›ºå®šçš„ $X=x$ï¼Œä½ çš„ç›®æ ‡æ˜¯æ‰¾åˆ° $f(x)$ æ¥æœ€å°åŒ– $\\mathbb{E}_{Y|X}[(Y - f(x))^2 | X=x]$ã€‚\n",
    "* **å›æƒ³ä¸€ä¸‹**ï¼šå¯¹äºä¸€ä¸ªéšæœºå˜é‡ $Z$ å’Œä¸€ä¸ªå¸¸æ•° $c$ï¼Œ$\\mathbb{E}[(Z - c)^2]$ åœ¨ $c$ ç­‰äºä»€ä¹ˆæ—¶å–å¾—æœ€å°å€¼ï¼Ÿï¼ˆæç¤ºï¼šæ±‚å¯¼å¹¶è®¾ä¸º0ï¼‰ã€‚å°†è¿™ä¸ªç»“è®ºåº”ç”¨åˆ° $Z=Y|X=x$ å’Œ $c=f(x)$ ä¸Šã€‚\n",
    "\n",
    "**Q2: ä¸ºåˆ†ç±»é—®é¢˜å†™å‡º EPR çš„ä¸€èˆ¬è¡¨è¾¾å¼ï¼ˆç®€åŒ–ç‰ˆï¼‰ã€‚**\n",
    "* **æç¤º**ï¼šè¿™é“é¢˜æ˜¯ Q1 çš„åˆ†ç±»ç‰ˆæœ¬ã€‚EPR çš„ä¸€èˆ¬å®šä¹‰æ˜¯ $\\mathbb{E}_{X,C}[L(C, \\hat{c}(X))]$ï¼Œå…¶ä¸­ $C$ æ˜¯çœŸå®ç±»åˆ«ï¼Œ$X$ æ˜¯æ•°æ®ã€‚\n",
    "* **å…³é”®æ€è·¯**ï¼šå†æ¬¡ä½¿ç”¨å…¨æœŸæœ›å®šå¾‹ï¼š$\\mathbb{E}_{X,C}[\\dots] = \\mathbb{E}_X[\\mathbb{E}_{C|X}[\\dots | X]]$ã€‚å°†è¿™ä¸ªæœŸæœ›å†™æˆä¸€ä¸ªç§¯åˆ†ï¼ˆå¯¹ $x$ï¼‰å’Œä¸€ä¸ªæ±‚å’Œï¼ˆå¯¹ $C$ çš„ $K$ ä¸ªç±»åˆ«ï¼‰çš„å½¢å¼ã€‚\n",
    "* **æœ€ç»ˆå½¢å¼**ï¼šä½ çš„è¡¨è¾¾å¼åº”è¯¥ä¼šæ¶‰åŠåˆ° $x$ çš„æ¦‚ç‡å¯†åº¦ $p(x)$ å’Œç»™å®š $x$ æ—¶ç±»åˆ« $c$ çš„æ¡ä»¶æ¦‚ç‡ $P(C=k | X=x)$ã€‚\n",
    "\n",
    "**Q3: æ‰¾åˆ° 0-1 æŸå¤±å‡½æ•° $L(c, \\hat{c}) = \\mathbb{I}(c \\neq \\hat{c})$ çš„æœ€ä¼˜åˆ†ç±»å™¨ï¼ˆè´å¶æ–¯åˆ†ç±»å™¨ï¼‰ã€‚**\n",
    "* **æç¤º**ï¼šä» Q2 çš„ç­”æ¡ˆå‡ºå‘ã€‚å¯¹äºä¸€ä¸ª*å›ºå®š*çš„ $x$ï¼Œä½ çš„ç›®æ ‡æ˜¯é€‰æ‹©ä¸€ä¸ªç±»åˆ« $\\hat{c}(x)$ æ¥æœ€å°åŒ–*æ¡ä»¶é£é™©*ï¼š\n",
    "    $\\mathbb{E}_{C|X}[L(C, \\hat{c}(x)) | X=x]$ã€‚\n",
    "* **å…³é”®æ€è·¯**ï¼šå°† 0-1 æŸå¤±ä»£å…¥ï¼š\n",
    "    $\\mathbb{E}_{C|X}[\\mathbb{I}(C \\neq \\hat{c}(x)) | X=x] = \\sum_{k=1}^K \\mathbb{I}(k \\neq \\hat{c}(x)) P(C=k | X=x)$\n",
    "* **æ€è€ƒ**ï¼šè¿™ä¸ªæ±‚å’Œç­‰äº $P(C \\neq \\hat{c}(x) | X=x)$ã€‚æœ€å°åŒ– $P(C \\neq \\hat{c}(x) | X=x)$ ç­‰ä»·äºæœ€å¤§åŒ–ä»€ä¹ˆï¼Ÿ\n",
    "\n",
    "---\n",
    "\n",
    "### Gaussian Mixture Models (GMM)\n",
    "\n",
    "**Q4: ç¼–å†™ä¸€ä¸ª Python å‡½æ•°æ¥ä» GMM ä¸­é‡‡æ ·ã€‚**\n",
    "* **æç¤º**ï¼šè¿™æ˜¯ä¸€ä¸ªä¸¤æ­¥é‡‡æ ·è¿‡ç¨‹ï¼š\n",
    "    1.  **é€‰æ‹©åˆ†é‡ (Component)**ï¼šæ ¹æ®å…ˆéªŒæ¦‚ç‡ $\\pi = [\\pi_1, \\ldots, \\pi_K]$ æŠ½æ ·ä¸€ä¸ªç±»åˆ« $k$ã€‚`numpy.random.choice` åœ¨è¿™é‡Œä¼šå¾ˆæœ‰ç”¨ã€‚\n",
    "    2.  **ä»åˆ†é‡ä¸­é‡‡æ ·**ï¼šä¸€æ—¦ä½ é€‰å®šäº†ç±»åˆ« $k$ï¼Œå°±ä»å¯¹åº”çš„å¤šå…ƒé«˜æ–¯åˆ†å¸ƒ $\\mathcal{N}(x|\\mu_k, \\Sigma_k)$ ä¸­æŠ½æ ·ä¸€ä¸ªæ•°æ®ç‚¹ $x$ã€‚`numpy.random.multivariate_normal` æ˜¯ä½ éœ€è¦çš„å‡½æ•°ã€‚\n",
    "* **å‡½æ•°ç»“æ„**ï¼šä½ çš„å‡½æ•°åº”è¯¥æ¥å— $\\pi$, $\\mu$ åˆ—è¡¨, $\\Sigma$ åˆ—è¡¨ä½œä¸ºè¾“å…¥ï¼Œå¹¶è¿”å›ä¸€ä¸ª $x$ å‘é‡åŠå…¶å¯¹åº”çš„ç±»åˆ« $k$ã€‚\n",
    "\n",
    "**Q5: é‡‡æ · 1000 ä¸ªç‚¹å¹¶ç”¨ matplotlib ç»˜å›¾ã€‚**\n",
    "* **æç¤º**ï¼š\n",
    "    1.  å®šä¹‰ Q5 ä¸­ç»™å‡ºçš„ $\\pi$, $\\mu$ å’Œ $\\Sigma$ å‚æ•°ã€‚\n",
    "    2.  è°ƒç”¨ä½  Q4 çš„å‡½æ•° 1000 æ¬¡ï¼Œå°† $x$ æ ·æœ¬ï¼ˆ2D å‘é‡ï¼‰å’Œå®ƒä»¬å¯¹åº”çš„ç±»åˆ« $c$ (1, 2, or 3) å­˜å‚¨åœ¨åˆ—è¡¨ä¸­ã€‚\n",
    "    3.  ä½¿ç”¨ `matplotlib.pyplot.scatter` ç»˜åˆ¶ $x$ æ ·æœ¬ã€‚\n",
    "    4.  **å…³é”®**ï¼šä½¿ç”¨ `c=` å‚æ•°ï¼Œå¹¶å°†ä½ å­˜å‚¨çš„ç±»åˆ«åˆ—è¡¨ï¼ˆ1, 2, 3ï¼‰ä¼ ç»™å®ƒï¼Œè¿™æ · `matplotlib` å°±ä¼šè‡ªåŠ¨ä¸ºä¸åŒç±»åˆ«çš„ç‚¹ç€ä¸Šä¸åŒçš„é¢œè‰²ã€‚\n",
    "\n",
    "**Q6: è®¡ç®—è¯¥æ¨¡å‹çš„åéªŒæ¦‚ç‡ã€‚**\n",
    "* **æç¤º**ï¼šè¿™é“é¢˜æ˜¯è´å¶æ–¯å®šç† (Bayes' theorem) çš„ç›´æ¥åº”ç”¨ã€‚\n",
    "* **ç›®æ ‡**ï¼šä½ è¦æ±‚çš„æ˜¯ $P(C=k | X=x)$ã€‚\n",
    "* **å…¬å¼**ï¼š$P(C=k | X=x) = \\frac{p(x|C=k) P(C=k)}{p(x)}$\n",
    "    * $P(C=k)$ å°±æ˜¯ $\\pi_k$ã€‚\n",
    "    * $p(x|C=k)$ æ˜¯ç¬¬ $k$ ä¸ªé«˜æ–¯åˆ†å¸ƒçš„æ¦‚ç‡å¯†åº¦å‡½æ•° (PDF)ï¼Œ$\\mathcal{N}(x|\\mu_k, \\Sigma_k)$ã€‚\n",
    "    * $p(x)$ æ˜¯è¾¹é™…æ¦‚ç‡ï¼Œç­‰äº $\\sum_{j=1}^K p(x|C=j) P(C=j)$ã€‚\n",
    "* **æ³¨æ„**ï¼šä½ ä¸éœ€è¦è®¡ç®—é«˜æ–¯ PDFï¼Œä½†ä½ éœ€è¦çŸ¥é“å®ƒçš„å½¢å¼ã€‚`scipy.stats.multivariate_normal.pdf` å¯èƒ½ä¼šæœ‰ç”¨ã€‚\n",
    "\n",
    "**Q7A: è§£æè®¡ç®—ç±»åˆ« 1 å’Œç±»åˆ« 2 ä¹‹é—´çš„å†³ç­–è¾¹ç•Œã€‚**\n",
    "* **æç¤º**ï¼šå†³ç­–è¾¹ç•Œå®šä¹‰ä¸º $P(C=1 | x) = P(C=2 | x)$ã€‚\n",
    "* **å…³é”®æ€è·¯**ï¼šä½¿ç”¨ Q6 çš„ç»“æœï¼Œè¿™ä¸ªç­‰å¼å¯ä»¥ç®€åŒ–ä¸ºï¼š\n",
    "    $p(x|C=1) P(C=1) = p(x|C=2) P(C=2)$\n",
    "    $\\implies p(x|C=1) \\pi_1 = p(x|C=2) \\pi_2$\n",
    "* **ç®€åŒ–**ï¼šä¸ºäº†æ›´å®¹æ˜“è®¡ç®—ï¼Œå–å¯¹æ•°ï¼š\n",
    "    $\\log p(x|C=1) + \\log \\pi_1 = \\log p(x|C=2) + \\log \\pi_2$\n",
    "* **é‡è¦è§‚å¯Ÿ**ï¼šåœ¨ Q5 çš„è®¾å®šä¸­ï¼Œ$\\Sigma_1 = \\Sigma_2 = I$ (å•ä½çŸ©é˜µ)ã€‚å½“ä½ ä»£å…¥é«˜æ–¯ PDF çš„å¯¹æ•°å½¢å¼ $\\log \\mathcal{N}(x|\\mu_k, \\Sigma_k) = -\\frac{1}{2}(x-\\mu_k)^T \\Sigma_k^{-1} (x-\\mu_k) - \\dots$ æ—¶ï¼ŒåŒ…å« $x$ çš„äºŒæ¬¡é¡¹ï¼ˆ$x^T x$ï¼‰ä¼šç›¸äº’æŠµæ¶ˆã€‚\n",
    "* **ç»“æœ**ï¼šä½ æœ€ç»ˆä¼šå¾—åˆ°ä¸€ä¸ªå…³äº $x$ (å³ $x_1, x_2$) çš„**çº¿æ€§æ–¹ç¨‹**ã€‚\n",
    "\n",
    "**Q7B: ç»˜åˆ¶ Q7A çš„æœ€ä¼˜å†³ç­–è¾¹ç•Œã€‚**\n",
    "* **æç¤º**ï¼šåœ¨ Q5 çš„è®¾å®šä¸‹ï¼Œå†³ç­–è¾¹ç•Œæ˜¯ç›´çº¿ã€‚ä½ æœ‰ä¸¤ç§æ–¹æ³•ï¼š\n",
    "    1.  **è§£ææ³•**ï¼šä» Q7A è§£å‡º $x_2$ ä½œä¸º $x_1$ çš„å‡½æ•°ï¼ˆå³ $x_2 = a x_1 + b$ï¼‰ï¼Œç„¶åç”¨ `plt.plot` ç”»å‡ºè¿™æ¡çº¿ã€‚ä½ éœ€è¦å¯¹ (1 vs 2), (2 vs 3), (1 vs 3) éƒ½è¿™æ ·åšã€‚\n",
    "    2.  **ç½‘æ ¼æ³• (Meshgrid)**ï¼ˆæ›´é€šç”¨ï¼Œä¹Ÿé€‚ç”¨äº Q10ï¼‰ï¼š\n",
    "        * åˆ›å»ºä¸€ä¸ªè¦†ç›–æ•°æ®èŒƒå›´çš„ `np.meshgrid`ã€‚\n",
    "        * åœ¨ç½‘æ ¼çš„*æ¯ä¸ª*ç‚¹ä¸Šï¼Œè®¡ç®— Q6 çš„ä¸‰ä¸ªåéªŒæ¦‚ç‡ $P(C=k|x)$ã€‚\n",
    "        * æ‰¾å‡ºåœ¨æ¯ä¸ªç‚¹ä¸Šå“ªä¸ª $k$ çš„åéªŒæ¦‚ç‡æœ€å¤§ï¼ˆ`np.argmax`ï¼‰ã€‚\n",
    "        * ä½¿ç”¨ `plt.contour` æˆ– `plt.contourf` æ ¹æ®â€œè·èƒœâ€çš„ç±»åˆ«æ¥ç»˜åˆ¶ç­‰é«˜çº¿å›¾ï¼Œè¿™ä¼šè‡ªåŠ¨æ˜¾ç¤ºå†³ç­–è¾¹ç•Œã€‚\n",
    "\n",
    "**Q8: è®¡ç®— Q5 æ•°æ®é›†ä¸Šè´å¶æ–¯åˆ†ç±»å™¨çš„é”™è¯¯ç‡ã€‚**\n",
    "* **æç¤º**ï¼šä½ éœ€è¦æ¯”è¾ƒ*çœŸå®ç±»åˆ«*å’Œ*è´å¶æ–¯é¢„æµ‹ç±»åˆ«*ã€‚\n",
    "* **æ­¥éª¤**ï¼š\n",
    "    1.  éå†ä½ åœ¨ Q5 ä¸­ç”Ÿæˆçš„ 1000 ä¸ªæ ·æœ¬ $(x_i, c_i)$ã€‚\n",
    "    2.  å¯¹äºæ¯ä¸ª $x_i$ï¼Œä½¿ç”¨ Q6 çš„å…¬å¼è®¡ç®—ä¸‰ä¸ªåéªŒæ¦‚ç‡ $P(C=k | x_i)$ã€‚\n",
    "    3.  ä½ çš„*é¢„æµ‹* $\\hat{c}(x_i)$ æ˜¯å…·æœ‰æœ€é«˜åéªŒæ¦‚ç‡çš„ç±»åˆ« $k$ï¼š$\\hat{c}(x_i) = \\text{argmax}_k P(C=k | x_i)$ã€‚\n",
    "    4.  æ¯”è¾ƒ $\\hat{c}(x_i)$ å’Œ $c_i$ï¼ˆç”Ÿæˆæ•°æ®æ—¶çš„çœŸå®ç±»åˆ«ï¼‰ã€‚\n",
    "    5.  è®¡ç®— $\\hat{c}(x_i) \\neq c_i$ çš„æ ·æœ¬æ¯”ä¾‹ã€‚\n",
    "\n",
    "---\n",
    "\n",
    "### Heterogeneous Model (ä¸åŒåæ–¹å·®)\n",
    "\n",
    "**Q9: åæ–¹å·®çŸ©é˜µä¸åŒæ—¶ï¼Œç±»åˆ« 1 å’Œ 2 ä¹‹é—´çš„å†³ç­–è¾¹ç•Œæ˜¯ä»€ä¹ˆï¼Ÿ**\n",
    "* **æç¤º**ï¼šèµ·ç‚¹ä¸ Q7A å®Œå…¨ç›¸åŒï¼š\n",
    "    $\\log p(x|C=1) + \\log \\pi_1 = \\log p(x|C=2) + \\log \\pi_2$\n",
    "* **å…³é”®åŒºåˆ«**ï¼šè¿™ä¸€æ¬¡ï¼Œ$\\Sigma_1 \\neq \\Sigma_2$ã€‚å½“ä½ ä»£å…¥é«˜æ–¯ PDF çš„å¯¹æ•°æ—¶ï¼ŒäºŒæ¬¡é¡¹ $-\\frac{1}{2}(x-\\mu_k)^T \\Sigma_k^{-1} (x-\\mu_k)$ ä¸­ $x^T \\Sigma_k^{-1} x$ çš„éƒ¨åˆ†å°†**ä¸ä¼š**ç›¸äº’æŠµæ¶ˆã€‚\n",
    "* **ç»“æœ**ï¼šå†³ç­–è¾¹ç•Œçš„æ–¹ç¨‹å°†æ˜¯ä¸€ä¸ªå…³äº $x$ çš„**äºŒæ¬¡æ–¹ç¨‹** (quadratic)ã€‚ä½ ä¸éœ€è¦å°†å…¶è§£ä¸º $x_2 = f(x_1)$ï¼Œåªéœ€è¦å†™å‡ºè¿™ä¸ªäºŒæ¬¡æ–¹ç¨‹æœ¬èº«ï¼ˆæˆ–èƒ½ç”¨äºç»˜å›¾çš„å‡½æ•°å½¢å¼ï¼‰å³å¯ã€‚\n",
    "\n",
    "**Q10: ç»˜åˆ¶æ–°æ¨¡å‹çš„å†³ç­–è¾¹ç•Œå¹¶è®¨è®ºå·®å¼‚ã€‚**\n",
    "* **æç¤º**ï¼š\n",
    "    * **ç»˜å›¾**ï¼šåœ¨è¿™é‡Œï¼Œä½ *å¿…é¡»*ä½¿ç”¨ Q7B æç¤ºä¸­çš„â€œç½‘æ ¼æ³• (Meshgrid)â€ã€‚å› ä¸ºè¾¹ç•Œæ˜¯æ›²çº¿ï¼Œç”¨ `plt.contourf` æ˜¯æœ€ç®€å•å’Œæœ€å‡†ç¡®çš„æ–¹æ³•ã€‚\n",
    "    * **è®¨è®º**ï¼šæ¯”è¾ƒè¿™å¼ å›¾å’Œ Q7B çš„å›¾ã€‚\n",
    "        * Q7B çš„è¾¹ç•Œæ˜¯**ç›´çº¿**ï¼ˆå› ä¸º $\\Sigma_k$ ç›¸åŒï¼‰ã€‚\n",
    "        * Q10 çš„è¾¹ç•Œæ˜¯**æ›²çº¿**ï¼ˆå› ä¸º $\\Sigma_k$ ä¸åŒï¼‰ã€‚\n",
    "        * çœ‹çœ‹ $\\Sigma_2$ï¼ˆæ–¹å·®æ›´å¤§ï¼‰å’Œ $\\Sigma_3$ï¼ˆæœ‰åæ–¹å·®/ç›¸å…³æ€§ï¼‰æ˜¯å¦‚ä½•â€œå¼¯æ›²â€å†³ç­–è¾¹ç•Œçš„ã€‚\n",
    "\n",
    "**Q11: ä»æ–°æ¨¡å‹ä¸­é‡‡æ · 1000 ä¸ªç‚¹å¹¶ç»˜å›¾ã€‚**\n",
    "* **æç¤º**ï¼šè¿™é“é¢˜ä¸ Q5 å®Œå…¨ç›¸åŒã€‚åªéœ€ä½¿ç”¨ Q4 çš„å‡½æ•°ï¼Œä½†ä¼ å…¥ Q8 ä¹‹åå®šä¹‰çš„æ–° $\\pi$, $\\mu$ å’Œ $\\Sigma$ å‚æ•°å³å¯ã€‚\n",
    "\n",
    "**Q12: åœ¨è¿™ä¸ªæ–°æ•°æ®é›†ä¸Šè®¡ç®—è´å¶æ–¯åˆ†ç±»å™¨çš„é”™è¯¯ç‡ã€‚**\n",
    "* **æç¤º**ï¼šè¿™é“é¢˜ä¸ Q8 çš„æ­¥éª¤*å®Œå…¨ç›¸åŒ*ã€‚\n",
    "    1.  ä½¿ç”¨ Q11 ç”Ÿæˆçš„æ–°æ ·æœ¬ $(x_i, c_i)$ã€‚\n",
    "    2.  å¯¹äºæ¯ä¸ª $x_i$ï¼Œä½¿ç”¨*æ–°*çš„ $\\pi$, $\\mu$, $\\Sigma$ å‚æ•°è®¡ç®—åéªŒæ¦‚ç‡ $P(C=k | x_i)$ã€‚\n",
    "    3.  æ‰¾åˆ°é¢„æµ‹ $\\hat{c}(x_i) = \\text{argmax}_k P(C=k | x_i)$ã€‚\n",
    "    4.  è®¡ç®— $\\hat{c}(x_i) \\neq c_i$ çš„æ¯”ä¾‹ã€‚\n",
    "\n",
    "å¸Œæœ›è¿™äº›æç¤ºå¯¹ä½ æœ‰å¸®åŠ©ï¼\n",
    "\n",
    "å¦‚æœä½ åœ¨æŸä¸ªç‰¹å®šé—®é¢˜ä¸Šå¡ä½äº†ï¼Œå¯ä»¥å†å…·ä½“é—®æˆ‘ã€‚ä¾‹å¦‚ï¼šâ€œQ7A çš„ä»£æ•°è¿ç®—æˆ‘å¡ä½äº†ï¼Œèƒ½å†ç»™ç‚¹æç¤ºå—ï¼Ÿâ€"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29c6c25a-bda9-4df9-a0fa-251ad2666c6e",
   "metadata": {},
   "source": [
    "Here is the full translation of the hints for your Homework 2.\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ¤– Hw2 Hints\n",
    "\n",
    "Here are the hints for each question in your .ipynb file:\n",
    "\n",
    "### Part 1: Expected Prediction Risk (EPR)\n",
    "\n",
    "**Q1: Simplify the EPR expression and show it is minimized by the conditional mean.**\n",
    "* **Hint**: Use the Law of Total Expectation, writing the EPR as $\\text{EPR}[f] = \\mathbb{E}_X[\\mathbb{E}_{Y|X}[(Y - f(X))^2 | X]]$.\n",
    "* **Key Idea**: For a fixed $X=x$, your goal is to find the $f(x)$ that minimizes $\\mathbb{E}_{Y|X}[(Y - f(x))^2 | X=x]$.\n",
    "* **Recall**: For a random variable $Z$ and a constant $c$, when is $\\mathbb{E}[(Z - c)^2]$ minimized? (Hint: Take the derivative w.r.t. $c$ and set to 0). Apply this conclusion to $Z=Y|X=x$ and $c=f(x)$.\n",
    "\n",
    "**Q2: Write the general (simplified) expression for EPR in a classification problem.**\n",
    "* **Hint**: This is the classification version of Q1. The general definition of EPR is $\\mathbb{E}_{X,C}[L(C, \\hat{c}(X))]$, where $C$ is the true class and $X$ is the data.\n",
    "* **Key Idea**: Again, use the Law of Total Expectation: $\\mathbb{E}_{X,C}[\\dots] = \\mathbb{E}_X[\\mathbb{E}_{C|X}[\\dots | X]]$. Write this expectation as an integral (over $x$) and a summation (over the $K$ classes of $C$).\n",
    "* **Final Form**: Your expression should involve the probability density of $x$, $p(x)$, and the conditional probability of class $c$ given $x$, $P(C=k | X=x)$.\n",
    "\n",
    "**Q3: Find the optimal classifier (Bayes classifier) for the 0-1 loss function $L(c, \\hat{c}) = \\mathbb{I}(c \\neq \\hat{c})$.**\n",
    "* **Hint**: Start from your answer to Q2. For a *fixed* $x$, your goal is to choose a class $\\hat{c}(x)$ to minimize the *conditional risk*:\n",
    "    $\\mathbb{E}_{C|X}[L(C, \\hat{c}(x)) | X=x]$.\n",
    "* **Key Idea**: Substitute the 0-1 loss:\n",
    "    $\\mathbb{E}_{C|X}[\\mathbb{I}(C \\neq \\hat{c}(x)) | X=x] = \\sum_{k=1}^K \\mathbb{I}(k \\neq \\hat{c}(x)) P(C=k | X=x)$\n",
    "* **Think**: This sum is equal to $P(C \\neq \\hat{c}(x) | X=x)$. Minimizing $P(C \\neq \\hat{c}(x) | X=x)$ is equivalent to maximizing what?\n",
    "\n",
    "---\n",
    "\n",
    "### Gaussian Mixture Models (GMM)\n",
    "\n",
    "**Q4: Write a Python function to sample from a GMM.**\n",
    "* **Hint**: This is a two-step sampling process:\n",
    "    1.  **Choose a Component**: Sample a class $k$ according to the prior probabilities $\\pi = [\\pi_1, \\ldots, \\pi_K]$. `numpy.random.choice` will be very useful here.\n",
    "    2.  **Sample from the Component**: Once you've chosen class $k$, sample a data point $x$ from the corresponding multivariate Gaussian distribution $\\mathcal{N}(x|\\mu_k, \\Sigma_k)$. `numpy.random.multivariate_normal` is the function you need.\n",
    "* **Function Structure**: Your function should take $\\pi$, a list of $\\mu$, and a list of $\\Sigma$ as input, and return an $x$ vector and its corresponding class $k$.\n",
    "\n",
    "**Q5: Sample 1000 points and plot them using matplotlib.**\n",
    "* **Hint**:\n",
    "    1.  Define the $\\pi$, $\\mu$, and $\\Sigma$ parameters given in Q5.\n",
    "    2.  Call your Q4 function 1000 times, storing the $x$ samples (2D vectors) and their corresponding classes $c$ (1, 2, or 3) in lists.\n",
    "    3.  Use `matplotlib.pyplot.scatter` to plot the $x$ samples.\n",
    "    4.  **Key**: Use the `c=` argument and pass it your list of stored classes (1, 2, 3). `matplotlib` will then automatically color the points by class.\n",
    "\n",
    "**Q6: Calculate the posterior probability of the model.**\n",
    "* **Hint**: This is a direct application of Bayes' theorem.\n",
    "* **Objective**: You are asked to find $P(C=k | X=x)$.\n",
    "* **Formula**: $P(C=k | X=x) = \\frac{p(x|C=k) P(C=k)}{p(x)}$\n",
    "    * $P(C=k)$ is just $\\pi_k$.\n",
    "    * $p(x|C=k)$ is the probability density function (PDF) of the $k$-th Gaussian distribution, $\\mathcal{N}(x|\\mu_k, \\Sigma_k)$.\n",
    "    * $p(x)$ is the marginal probability (evidence), equal to $\\sum_{j=1}^K p(x|C=j) P(C=j)$.\n",
    "* **Note**: You don't need to *calculate* the Gaussian PDF by hand, but you need to know its form. `scipy.stats.multivariate_normal.pdf` will be useful.\n",
    "\n",
    "**Q7A: Analytically calculate the decision boundary between class 1 and class 2.**\n",
    "* **Hint**: The decision boundary is defined as the set of points $x$ where $P(C=1 | x) = P(C=2 | x)$.\n",
    "* **Key Idea**: Using the result from Q6, this equation simplifies to:\n",
    "    $p(x|C=1) P(C=1) = p(x|C=2) P(C=2)$\n",
    "    $\\implies p(x|C=1) \\pi_1 = p(x|C=2) \\pi_2$\n",
    "* **Simplify**: To make the calculation easier, take the logarithm of both sides:\n",
    "    $\\log p(x|C=1) + \\log \\pi_1 = \\log p(x|C=2) + \\log \\pi_2$\n",
    "* **Important Observation**: In the setup for Q5, $\\Sigma_1 = \\Sigma_2 = I$ (the identity matrix). When you substitute the log-form of the Gaussian PDF, $\\log \\mathcal{N}(x|\\mu_k, \\Sigma_k) = -\\frac{1}{2}(x-\\mu_k)^T \\Sigma_k^{-1} (x-\\mu_k) - \\dots$, the quadratic terms involving $x$ (i.e., $x^T x$) will cancel each other out.\n",
    "* **Result**: You will end up with a **linear equation** in terms of $x$ (i.e., $x_1, x_2$).\n",
    "\n",
    "**Q7B: Plot the optimal decision boundary from Q7A.**\n",
    "* **Hint**: Under the Q5 setup, the decision boundaries are lines. You have two methods:\n",
    "    1.  **Analytic Method**: Solve your equation from Q7A for $x_2$ as a function of $x_1$ (i.e., $x_2 = a x_1 + b$), then use `plt.plot` to draw this line. You'll need to do this for (1 vs 2), (2 vs 3), and (1 vs 3).\n",
    "    2.  **Meshgrid Method** (More general, and also works for Q10):\n",
    "        * Create an `np.meshgrid` that covers the range of your data.\n",
    "        * At *every* point on this grid, calculate the three posterior probabilities $P(C=k|x)$ from Q6.\n",
    "        * Find which $k$ has the largest posterior probability at each point (`np.argmax`).\n",
    "        * Use `plt.contour` or `plt.contourf` to plot the contour map based on the \"winning\" class. This will automatically show the decision boundaries.\n",
    "\n",
    "**Q8: Calculate the error rate of the Bayes classifier on the Q5 dataset.**\n",
    "* **Hint**: You need to compare the *true class* and the *Bayes predicted class*.\n",
    "* **Steps**:\n",
    "    1.  Iterate through the 1000 samples $(x_i, c_i)$ you generated in Q5.\n",
    "    2.  For each $x_i$, calculate the three posterior probabilities $P(C=k | x_i)$ using the formula from Q6.\n",
    "    3.  Your *prediction* $\\hat{c}(x_i)$ is the class $k$ with the highest posterior probability: $\\hat{c}(x_i) = \\text{argmax}_k P(C=k | x_i)$.\n",
    "    4.  Compare $\\hat{c}(x_i)$ with $c_i$ (the true class from data generation).\n",
    "    5.  Calculate the proportion of samples where $\\hat{c}(x_i) \\neq c_i$.\n",
    "\n",
    "---\n",
    "\n",
    "### Heterogeneous Model (Different Covariances)\n",
    "\n",
    "**Q9: When the covariance matrices are different, what is the decision boundary between class 1 and 2?**\n",
    "* **Hint**: The starting point is identical to Q7A:\n",
    "    $\\log p(x|C=1) + \\log \\pi_1 = \\log p(x|C=2) + \\log \\pi_2$\n",
    "* **Key Difference**: This time, $\\Sigma_1 \\neq \\Sigma_2$. When you substitute the log-PDF, the quadratic part $-\\frac{1}{2}(x-\\mu_k)^T \\Sigma_k^{-1} (x-\\mu_k)$ (specifically the $x^T \\Sigma_k^{-1} x$ term) will **not** cancel out.\n",
    "* **Result**: The equation for the decision boundary will be a **quadratic equation** in $x$. You don't need to solve it for $x_2 = f(x_1)$, just writing the quadratic equation itself (or a functional form usable for plotting) is sufficient.\n",
    "\n",
    "**Q10: Plot the new model's decision boundaries and discuss the differences.**\n",
    "* **Hint**:\n",
    "    * **Plotting**: Here, you *must* use the \"Meshgrid Method\" from the Q7B hint. Because the boundaries are curves, using `plt.contourf` is the easiest and most-accurate method.\n",
    "    * **Discussion**: Compare this plot to the plot from Q7B.\n",
    "        * The boundaries in Q7B were **lines** (because all $\\Sigma_k$ were identical).\n",
    "        * The boundaries in Q10 are **curves** (because the $\\Sigma_k$ are different).\n",
    "        * Look at how $\\Sigma_2$ (larger variance) and $\\Sigma_3$ (has covariance/correlation) \"bend\" the decision boundaries.\n",
    "\n",
    "**Q11: Sample 1000 points from the new model and plot them.**\n",
    "* **Hint**: This is identical to Q5. Just use your function from Q4, but pass in the *new* $\\pi$, $\\mu$, and $\\Sigma$ parameters defined after Q8.\n",
    "\n",
    "**Q12: Calculate the Bayes classifier error rate on this new dataset.**\n",
    "* **Hint**: The steps for this question are *exactly the same* as Q8.\n",
    "    1.  Use the new samples $(x_i, c_i)$ generated in Q11.\n",
    "    2.  For each $x_i$, calculate the posterior probabilities $P(C=k | x_i)$ using the *new* $\\pi$, $\\mu$, and $\\Sigma$ parameters.\n",
    "    3.  Find the prediction $\\hat{c}(x_i) = \\text{argmax}_k P(C=k | x_i)$.\n",
    "    4.  Calculate the proportion where $\\hat{c}(x_i) \\neq c_i$.\n",
    "\n",
    "Hope these hints are helpful!\n",
    "\n",
    "If you get stuck on a specific problem, feel free to ask me for more details. For example: \"I'm stuck on the algebra for Q7A, can you give me another hint?\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d09424da-8301-4243-8fe5-81f6916e1743",
   "metadata": {},
   "source": [
    "Yes, of course. Using $C_k$ to denote the event that the data point belongs to component $k$, the derivation is as follows:\n",
    "\n",
    "### Derivation using $C_k$ Notation\n",
    "\n",
    "1.  **Start with Bayes' Theorem:**\n",
    "    The formula for the posterior probability of component $k$ given the data $x$ is:\n",
    "    $$\n",
    "    P(C_k | X=x) = \\frac{p(x|C_k) P(C_k)}{p(x)}\n",
    "    $$\n",
    "\n",
    "2.  **Substitute the Numerator Components:**\n",
    "    We replace the general terms with the specific GMM parameters:\n",
    "    * **Prior $P(C_k)$:** This is the prior probability (mixture weight) of component $k$, which is $\\pi_k$.\n",
    "    * **Likelihood $p(x|C_k)$:** This is the probability density of $x$ given it came from component $k$, which is the Gaussian PDF $\\mathcal{N}(x|\\mu_k, \\Sigma_k)$.\n",
    "\n",
    "    The numerator becomes:\n",
    "    $$\n",
    "    \\text{Numerator} = \\pi_k \\mathcal{N}(x|\\mu_k, \\Sigma_k)\n",
    "    $$\n",
    "\n",
    "3.  **Substitute the Denominator (Evidence):**\n",
    "    The denominator $p(x)$ is the marginal probability of $x$. We find this by summing the joint probabilities over all possible components $j$ (from 1 to $K$) using the Law of Total Probability:\n",
    "    * The formula is: $p(x) = \\sum_{j=1}^K p(x|C_j) P(C_j)$\n",
    "    * Substituting the GMM parameters into this sum:\n",
    "    $$\n",
    "    p(x) = \\sum_{j=1}^K \\pi_j \\mathcal{N}(x|\\mu_j, \\Sigma_j)\n",
    "    $$\n",
    "\n",
    "4.  **Combine for the Final Formula:**\n",
    "    Finally, we place the numerator from step 2 and the denominator from step 3 back into the main Bayes' formula:\n",
    "\n",
    "    $$\n",
    "    P(C_k | X=x) = \\frac{\\pi_k \\mathcal{N}(x|\\mu_k, \\Sigma_k)}{\\sum_{j=1}^K \\pi_j \\mathcal{N}(x|\\mu_j, \\Sigma_j)}\n",
    "    $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d242f30-d6a9-4d05-b3d8-4c33ed575e99",
   "metadata": {},
   "source": [
    "Here is the concise derivation for the posterior probability using the notation $c_k$.\n",
    "\n",
    "The posterior probability $P(c_k | x)$ is the probability that a point $x$ belongs to component $k$. We find it using Bayes' theorem:\n",
    "\n",
    "$$\n",
    "P(c_k | x) = \\frac{p(x|c_k) P(c_k)}{p(x)}\n",
    "$$\n",
    "\n",
    "By substituting the specific components of the Gaussian Mixture Model, we get the final formula:\n",
    "\n",
    "$$\n",
    "P(c_k | x) = \\frac{\\pi_k \\mathcal{N}(x|\\mu_k, \\Sigma_k)}{\\sum_{j=1}^K \\pi_j \\mathcal{N}(x|\\mu_j, \\Sigma_j)}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### Derivation Steps\n",
    "\n",
    "1.  **Numerator:** The numerator is the joint probability $p(x, c_k)$.\n",
    "    * $P(c_k) = \\pi_k$ (The prior probability, or weight, of component $k$)\n",
    "    * $p(x|c_k) = \\mathcal{N}(x|\\mu_k, \\Sigma_k)$ (The likelihood of $x$ under component $k$'s Gaussian)\n",
    "    * Thus, $\\text{Numerator} = \\pi_k \\mathcal{N}(x|\\mu_k, \\Sigma_k)$\n",
    "\n",
    "2.  **Denominator:** The denominator $p(x)$ is the total probability (evidence) of $x$, found by summing the joint probabilities over all $K$ components.\n",
    "    * $p(x) = \\sum_{j=1}^K p(x, c_j) = \\sum_{j=1}^K p(x|c_j) P(c_j)$\n",
    "    * Thus, $\\text{Denominator} = \\sum_{j=1}^K \\pi_j \\mathcal{N}(x|\\mu_j, \\Sigma_j)$\n",
    "\n",
    "3.  **Result:** Combining the numerator and denominator gives the posterior probability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83ae087b-9819-4d15-a9df-c00ca0733611",
   "metadata": {},
   "source": [
    "### **Q6 Ans**\n",
    "Using Bayes' theorem, the posterior probability will be:\n",
    "\n",
    "$$\n",
    "P(c_k | x) = \\frac{p(x|c_k) P(c_k)}{p(x)}\n",
    "$$\n",
    "\n",
    "By substituting the specific components of the Gaussian Mixture Model :\n",
    "\n",
    "* $P(c_k) = \\pi_k$ (The prior probability, or weight, of component $k$)\n",
    "* $p(x|c_k) = \\mathcal{N}(x|\\mu_k, \\Sigma_k)$ (The likelihood of $x$ under component $k$'s Gaussian)\n",
    "\n",
    "* $p(x)$ is the total probability (evidence) of $x$, found by summing the joint probabilities over all $K$ components: $p(x) = \\sum_{j=1}^K p(x, c_j) = \\sum_{j=1}^K p(x|c_j) P(c_j)$\n",
    "\n",
    "Final formula will be:\n",
    "$$\n",
    "P(c_k | x) = \\frac{\\pi_k \\mathcal{N}(x|\\mu_k, \\Sigma_k)}{\\sum_{j=1}^K \\pi_j \\mathcal{N}(x|\\mu_j, \\Sigma_j)}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "809abe8d-2fc7-40ff-98ab-3706b99c5151",
   "metadata": {},
   "source": [
    "ä½ æå‡ºçš„è¿™ä¸€ç‚¹**éå¸¸æ­£ç¡®**ï¼Œè€Œä¸”æ˜¯ç†è§£è¿™ä¸ªé—®é¢˜çš„**æ ¸å¿ƒå…³é”®**ï¼\n",
    "\n",
    "ä½ å¼•ç”¨çš„å®šä¹‰æ˜¯ **Q7A** çš„**ç†è®ºå®šä¹‰**ï¼š\n",
    "$$\\text{Boundary between } i, j = \\{ x \\text{ | } P(C=i | x) = P(C=j | x) \\}$$\n",
    "\n",
    "è€Œæˆ‘ç»™ä½ çš„å‡½æ•° `predict_class_GMM` å’Œ `plot_decision_boundary` æ˜¯ **Q7B** å’Œ **Q10** çš„**è®¡ç®—å®ç°**ã€‚\n",
    "\n",
    "### ç†è®º vs. å®è·µ (ä¸ºä»€ä¹ˆæˆ‘çš„å‡½æ•°æ˜¯æ­£ç¡®çš„)\n",
    "\n",
    "ä½ å¯èƒ½ä¼šé—®ï¼šâ€œä½†ä½ çš„ `predict_class_GMM` å‡½æ•°å¹¶æ²¡æœ‰æ£€æŸ¥â€˜ç›¸ç­‰â€™ï¼Œå®ƒç”¨çš„æ˜¯ `np.argmax` (æ‰¾æœ€å¤§å€¼)ã€‚è¿™æ€ä¹ˆä¼šæ˜¯æ­£ç¡®çš„å‘¢ï¼Ÿâ€\n",
    "\n",
    "**è¿™å°±æ˜¯å®ƒä»¬ä¹‹é—´çš„è”ç³»ï¼š**\n",
    "\n",
    "1.  **è´å¶æ–¯åˆ†ç±»å™¨ (Bayes Classifier) (æ¥è‡ª Q3)**ï¼šè´å¶æ–¯åˆ†ç±»å™¨ï¼ˆBayes Classifierï¼‰è§„å®šï¼Œå¯¹äºä»»ä½•ç‚¹ $x$ï¼Œæˆ‘ä»¬åº”è¯¥é€‰æ‹©åéªŒæ¦‚ç‡**æœ€å¤§**çš„é‚£ä¸ªç±»åˆ«ã€‚\n",
    "    $$\\hat{c}(x) = \\text{argmax}_k P(C=k | x)$$\n",
    "    æˆ‘çš„ `predict_class_GMM` å‡½æ•°*æ­£æ˜¯*å®ç°äº†è¿™ä¸ªè´å¶æ–¯åˆ†ç±»å™¨ã€‚\n",
    "\n",
    "2.  **â€œè·èƒœåŒºåŸŸâ€**ï¼š\n",
    "\n",
    "      * **åŒºåŸŸ 1** æ˜¯æ‰€æœ‰ $x$ çš„é›†åˆï¼Œåœ¨è¿™äº› $x$ ä¸Šï¼Œç±»åˆ« 1 æ˜¯â€œè·èƒœè€…â€ï¼Œå³ $P(C=1 | x)$ æ˜¯æœ€å¤§çš„ã€‚\n",
    "      * **åŒºåŸŸ 2** æ˜¯æ‰€æœ‰ $x$ çš„é›†åˆï¼Œåœ¨è¿™äº› $x$ ä¸Šï¼Œç±»åˆ« 2 æ˜¯â€œè·èƒœè€…â€ï¼Œå³ $P(C=2 | x)$ æ˜¯æœ€å¤§çš„ã€‚\n",
    "      * ...ä»¥æ­¤ç±»æ¨ã€‚\n",
    "\n",
    "3.  **å†³ç­–è¾¹ç•Œ (Decision Boundary)**ï¼š\n",
    "    å†³ç­–è¾¹ç•Œå°±æ˜¯\\*\\*â€œè·èƒœåŒºåŸŸâ€å‘ç”Ÿæ”¹å˜\\*\\*çš„é‚£æ¡çº¿ã€‚\n",
    "\n",
    "      * ä»â€œåŒºåŸŸ 1â€ ç§»åŠ¨åˆ° â€œåŒºåŸŸ 2â€ çš„é‚£æ¡çº¿åœ¨å“ªé‡Œï¼Ÿ\n",
    "      * å®ƒ**æ°å¥½**åœ¨ $P(C=1 | x) = P(C=2 | x)$ çš„åœ°æ–¹ï¼\n",
    "      * åœ¨è¿™æ¡çº¿çš„ä¸€ä¾§ï¼Œ $P(C=1 | x) > P(C=2 | x)$ (åŒºåŸŸ 1 è·èƒœ)ã€‚\n",
    "      * åœ¨è¿™æ¡çº¿çš„å¦ä¸€ä¾§ï¼Œ $P(C=2 | x) > P(C=1 | x)$ (åŒºåŸŸ 2 è·èƒœ)ã€‚\n",
    "\n",
    "**æ€»ç»“ï¼š**\n",
    "\n",
    "  * ä½ å¼•ç”¨çš„**ç†è®ºå®šä¹‰** (Q7A) æ˜¯ä¸€ä¸ª**è§£ææ–¹ç¨‹**ï¼Œå®ƒå®šä¹‰äº†é‚£æ¡çº¿ã€‚\n",
    "  * æˆ‘çš„**ç»˜å›¾å‡½æ•°** (Q7B / Q10) æ˜¯ä¸€ä¸ª**è®¡ç®—æ–¹æ³•**ã€‚å®ƒé€šè¿‡åœ¨æ•°åƒä¸ªç‚¹ä¸Šæ‰¾åˆ°â€œè·èƒœè€…â€ï¼ˆä½¿ç”¨ `argmax`ï¼‰ï¼Œç„¶åè®© `plt.contourf` è‡ªåŠ¨â€œç”»å‡ºâ€é‚£äº›â€œè·èƒœåŒºåŸŸâ€å‘ç”Ÿå˜åŒ–çš„è¾¹ç•Œã€‚\n",
    "\n",
    "è¿™ä¸¤ä¸ªæ–¹æ³•æœ€ç»ˆç”»å‡ºçš„æ˜¯**åŒä¸€æ¡çº¿**ã€‚\n",
    "\n",
    "-----\n",
    "\n",
    "### ä¼˜åŒ–çš„ä»£ç  (å¸¦è‹±æ–‡æ³¨é‡Š)\n",
    "\n",
    "åœ¨æˆ‘çš„ `predict_class_GMM` å‡½æ•°ä¸­ï¼Œæˆ‘è¿˜åšäº†ä¸€ä¸ª**ä¼˜åŒ–**ã€‚\n",
    "\n",
    "æ ¹æ®è´å¶æ–¯å®šç†ï¼š\n",
    "$$P(C=k | x) = \\frac{\\pi_k \\mathcal{N}(x|\\mu_k, \\Sigma_k)}{p(x)}$$\n",
    "\n",
    "å½“æˆ‘è¦æ‰¾ $\\text{argmax}_k$ æ—¶ï¼Œåˆ†æ¯ $p(x)$ å¯¹äºæ‰€æœ‰ $k$ éƒ½æ˜¯**ç›¸åŒ**çš„ã€‚è¿™æ„å‘³ç€å®ƒ**ä¸ä¼š**æ”¹å˜å“ªä¸ª $k$ æ˜¯æœ€å¤§çš„ã€‚\n",
    "\n",
    "æ‰€ä»¥ï¼Œä¸ºäº†**é€Ÿåº¦** (å°¤å…¶æ˜¯åœ¨ç½‘æ ¼ä¸Šè®¡ç®—æ•°åƒä¸ªç‚¹æ—¶)ï¼Œæˆ‘ä»¬å¯ä»¥**å¿½ç•¥**åˆ†æ¯ $p(x)$ï¼Œåªæ¯”è¾ƒåˆ†å­ï¼ˆæˆ‘ç§°ä¹‹ä¸º `score`ï¼‰ï¼š\n",
    "\n",
    "$$\\text{argmax}_k P(C=k | x) = \\text{argmax}_k \\frac{\\pi_k \\mathcal{N}(x|\\mu_k, \\Sigma_k)}{p(x)} = \\text{argmax}_k [ \\pi_k \\mathcal{N}(x|\\mu_k, \\Sigma_k) ]$$\n",
    "\n",
    "è¿™å°±æ˜¯ä¸ºä»€ä¹ˆæˆ‘çš„ `predict_class_GMM` å‡½æ•°é•¿è¿™ä¸ªæ ·å­ã€‚å®ƒæ›´é«˜æ•ˆï¼Œå¹¶ä¸”ç»“æœå®Œå…¨æ­£ç¡®ã€‚\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import multivariate_normal\n",
    "\n",
    "# --- Core GMM Helper Functions ---\n",
    "\n",
    "def likelihood_GMM(pis, means, covs, component_k, sample_x):\n",
    "    \"\"\"\n",
    "    Computes the likelihood p(x | C=k)\n",
    "    This is the PDF of the k-th Gaussian component.\n",
    "    \"\"\"\n",
    "    pdf_k = multivariate_normal(mean=means[component_k], cov=covs[component_k])\n",
    "    likelihood = pdf_k.pdf(sample_x)\n",
    "    return likelihood\n",
    "\n",
    "def predict_class_GMM(pis, means, covs, n_components, sample_x):\n",
    "    \"\"\"\n",
    "    Finds the \"winning\" class (Bayes Classifier) for a single point x.\n",
    "    This implements argmax_k P(C=k | x).\n",
    "    \n",
    "    We optimize by only calculating the numerator of the posterior,\n",
    "    p(x|C=k) * p(C=k), since the denominator p(x) is constant\n",
    "    for all k and doesn't affect the argmax.\n",
    "    \"\"\"\n",
    "    # Create an array to hold the \"score\" (numerator) for each class\n",
    "    scores = np.zeros(n_components)\n",
    "    \n",
    "    for k in range(n_components):\n",
    "        # score_k = p(C=k) * p(x | C=k)\n",
    "        scores[k] = pis[k] * likelihood_GMM(pis, means, covs, k, sample_x)\n",
    "        \n",
    "    # Return the *index* (0, 1, or 2) of the class with the highest score\n",
    "    return np.argmax(scores)\n",
    "\n",
    "# --- Plotting Function (for Q7B / Q10) ---\n",
    "\n",
    "def plot_decision_boundary(pis, means, covs, n_components, samples_data, labels_data):\n",
    "    \"\"\"\n",
    "    Plots the decision boundaries by:\n",
    "    1. Creating a fine meshgrid of points.\n",
    "    2. Predicting the \"winning\" class (using predict_class_GMM) for EACH point.\n",
    "    3. Using plt.contourf to plot the regions.\n",
    "    The boundary line is automatically drawn where the \"winning\" class changes.\n",
    "    \"\"\"\n",
    "    \n",
    "    # 1. Define the boundaries of the plot\n",
    "    x_min, x_max = samples_data[:, 0].min() - 1, samples_data[:, 0].max() + 1\n",
    "    y_min, y_max = samples_data[:, 1].min() - 1, samples_data[:, 1].max() + 1\n",
    "    \n",
    "    # 2. Create the meshgrid\n",
    "    h = 0.05 # Step size in the mesh\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                         np.arange(y_min, y_max, h))\n",
    "\n",
    "    # 3. Predict the class for every point in the grid\n",
    "    # np.c_[xx.ravel(), yy.ravel()] flattens the grid into (N, 2) array of coordinates\n",
    "    grid_points = np.c_[xx.ravel(), yy.ravel()]\n",
    "    \n",
    "    # Run prediction on all points. This might take a moment.\n",
    "    Z = np.array([predict_class_GMM(pis, means, covs, n_components, pt) for pt in grid_points])\n",
    "    \n",
    "    # Reshape the results (Z) back to the grid shape\n",
    "    Z = Z.reshape(xx.shape)\n",
    "\n",
    "    # 4. Plot the contour (regions) and the data points\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    \n",
    "    # Plot the filled contour map\n",
    "    plt.contourf(xx, yy, Z, alpha=0.3, cmap='viridis')\n",
    "    \n",
    "    # Plot the original data points on top\n",
    "    plt.scatter(samples_data[:, 0], samples_data[:, 1], c=labels_data, \n",
    "                cmap='viridis', edgecolors='k', s=25, alpha=0.7)\n",
    "    \n",
    "    plt.title(\"GMM Decision Boundaries and Data\")\n",
    "    plt.xlabel(\"X1\")\n",
    "    plt.ylabel(\"X2\")\n",
    "    plt.show()\n",
    "\n",
    "# --- Example Usage (Using Q5 parameters) ---\n",
    "\n",
    "# Q5 Parameters\n",
    "K = 3\n",
    "pis_q5 = np.array([0.3, 0.4, 0.3]);\n",
    "means_q5 = np.array([\n",
    "    [0, 0],\n",
    "    [3, 0],\n",
    "    [0, 3]\n",
    "])\n",
    "covs_q5 = np.array([\n",
    "    [[1, 0], [0, 1]],\n",
    "    [[1, 0], [0, 1]],\n",
    "    [[1, 0], [0, 1]]\n",
    "])\n",
    "\n",
    "# Assume 'samples_data' and 'labels_data' are generated from your Q5 sampling code\n",
    "# (e.g., samples_data, labels_data = generate_N_GMM_samples(pis_q5, means_q5, covs_q5, K, 1000))\n",
    "\n",
    "# --- MOCK DATA (if you haven't generated samples yet, for testing) ---\n",
    "# If you don't have the sampling functions handy, you can uncomment these lines\n",
    "# to see the plot function work with mock data.\n",
    "# print(\"Warning: Using MOCK data for plotting.\")\n",
    "# samples_data = np.random.rand(100, 2) * 5\n",
    "# labels_data = np.random.randint(0, 3, 100)\n",
    "# --- End Mock Data ---\n",
    "\n",
    "\n",
    "# To run:\n",
    "# 1. Generate your data first:\n",
    "# samples_data, labels_data = generate_N_GMM_samples(pis_q5, means_q5, covs_q5, K, 1000)\n",
    "\n",
    "# 2. Then, call the plot function:\n",
    "# plot_decision_boundary(pis_q5, means_q5, covs_q5, K, samples_data, labels_data)\n",
    "\n",
    "print(\"Functions are defined. Ready to generate samples and plot.\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7deb0f07-867a-4d86-9c68-b915800896eb",
   "metadata": {},
   "source": [
    "è¿™æ˜¯ä¸€ä¸ªéå¸¸æ£’çš„é—®é¢˜ï¼Œå®ƒå®Œç¾åœ°è¿æ¥äº†ç†è®ºï¼ˆQ7Aï¼‰å’Œå®è·µï¼ˆQ7Bï¼‰ã€‚\n",
    "\n",
    "-----\n",
    "\n",
    " \n",
    "\n",
    "-----\n",
    "\n",
    "### Q7B: ç»˜åˆ¶æœ€ä¼˜å†³ç­–è¾¹ç•Œ\n",
    "\n",
    "Q7B è¦æ±‚æˆ‘ä»¬ç»˜åˆ¶**æ¨¡å‹**çš„**æœ€ä¼˜**å†³ç­–è¾¹ç•Œï¼Œè¿™æ„å‘³ç€æˆ‘ä»¬éœ€è¦ç»˜åˆ¶**æ‰€æœ‰ä¸‰ä¸ª**è¾¹ç•Œï¼ˆ1-2, 1-3, 2-3ï¼‰ã€‚\n",
    "\n",
    "æˆ‘ä»¬å°†ä½¿ç”¨ä½ åœ¨ä¹‹å‰é—®é¢˜ä¸­æ„å»ºçš„ `predict_class_GMM` å‡½æ•°å’Œâ€œç½‘æ ¼â€(Meshgrid)æ–¹æ³•æ¥ç»˜åˆ¶**è´å¶æ–¯åˆ†ç±»å™¨**çš„â€œè·èƒœåŒºåŸŸâ€ã€‚æ­£å¦‚æˆ‘ä»¬è®¨è®ºè¿‡çš„ï¼Œè¿™äº›åŒºåŸŸçš„è¾¹ç¼˜**å°±æ˜¯** Q7A ä¸­å®šä¹‰çš„è§£æè¾¹ç•Œã€‚\n",
    "\n",
    "ä¸‹é¢çš„ä»£ç ç»˜åˆ¶äº†å†³ç­–åŒºåŸŸï¼Œå¹¶ä¸”**é¢å¤–**ç»˜åˆ¶äº†æˆ‘ä»¬åœ¨ Q7A ä¸­è®¡ç®—å‡ºçš„çº¢è‰²è™šçº¿ï¼Œä»¥è¯æ˜å®ƒä»¬æ˜¯å®Œå…¨å»åˆçš„ã€‚\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import multivariate_normal\n",
    "\n",
    "# --- å‡è®¾ä½ å·²ç»å®šä¹‰äº†è¿™äº›å‡½æ•° ---\n",
    "# (å¦‚æœæ²¡æœ‰ï¼Œè¯·ä»æˆ‘ä»¬ä¹‹å‰çš„å¯¹è¯ä¸­å¤åˆ¶å®ƒä»¬)\n",
    "\n",
    "def gaussianMixture(pis, means, covs, n_components):\n",
    "    # (Selects component k based on pis)\n",
    "    index_k = np.arange(n_components) \n",
    "    component_k = np.random.choice(index_k, p=pis)\n",
    "    \n",
    "    # (Samples from the chosen component)\n",
    "    component_mean = means[component_k];\n",
    "    component_cov = covs[component_k];\n",
    "    sample = np.random.multivariate_normal(component_mean, component_cov)\n",
    "    \n",
    "    return sample, component_k\n",
    "\n",
    "def generate_N_GMM_samples(pis, means, covs, n_components, n_samples):\n",
    "    samples_array = np.zeros((n_samples, 2))\n",
    "    labels_array = np.zeros(n_samples, dtype=int)\n",
    "    for i in range(n_samples): \n",
    "        random_sample, random_label = gaussianMixture(pis, means, covs, n_components)\n",
    "        samples_array[i] = random_sample\n",
    "        labels_array[i] = random_label\n",
    "    return samples_array, labels_array\n",
    "\n",
    "def likelihood_GMM(pis, means, covs, component_k, sample_x):\n",
    "    # Computes p(x | C=k)\n",
    "    pdf_k = multivariate_normal(mean=means[component_k], cov=covs[component_k])\n",
    "    likelihood = pdf_k.pdf(sample_x)\n",
    "    return likelihood\n",
    "\n",
    "def predict_class_GMM(pis, means, covs, n_components, sample_x):\n",
    "    # Computes argmax_k [ p(C=k) * p(x | C=k) ]\n",
    "    scores = np.zeros(n_components)\n",
    "    for k in range(n_components):\n",
    "        scores[k] = pis[k] * likelihood_GMM(pis, means, covs, k, sample_x)\n",
    "    return np.argmax(scores)\n",
    "# ----------------------------------------\n",
    "\n",
    "\n",
    "def plot_decision_boundary(pis, means, covs, n_components, samples_data, labels_data):\n",
    "    \"\"\"\n",
    "    Plots the GMM decision boundaries using a meshgrid.\n",
    "    (This is the main function for Q7B)\n",
    "    \"\"\"\n",
    "    \n",
    "    # 1. Define the boundaries of the plot\n",
    "    x_min, x_max = samples_data[:, 0].min() - 1, samples_data[:, 0].max() + 1\n",
    "    y_min, y_max = samples_data[:, 1].min() - 1, samples_data[:, 1].max() + 1\n",
    "    \n",
    "    # 2. Create the meshgrid\n",
    "    h = 0.05 # Step size in the mesh\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                         np.arange(y_min, y_max, h))\n",
    "\n",
    "    # 3. Predict the class for every point in the grid\n",
    "    grid_points = np.c_[xx.ravel(), yy.ravel()]\n",
    "    \n",
    "    print(\"Predicting on grid points... (this may take a moment)\")\n",
    "    Z = np.array([predict_class_GMM(pis, means, covs, n_components, pt) for pt in grid_points])\n",
    "    print(\"Prediction complete.\")\n",
    "    \n",
    "    # Reshape the results (Z) back to the grid shape\n",
    "    Z = Z.reshape(xx.shape)\n",
    "\n",
    "    # 4. Plot the contour (regions) and the data points\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    \n",
    "    # Plot the filled contour map\n",
    "    plt.contourf(xx, yy, Z, alpha=0.3, cmap='viridis')\n",
    "    \n",
    "    # Plot the original data points on top\n",
    "    plt.scatter(samples_data[:, 0], samples_data[:, 1], c=labels_data, \n",
    "                cmap='viridis', edgecolors='k', s=25, alpha=0.7)\n",
    "    \n",
    "    # --- Q7B: Plotting the Q7A Analytical Line ---\n",
    "    # This proves that our analytical calculation matches the plot!\n",
    "    \n",
    "    # Calculate the exact x_1 value from Q7A\n",
    "    x_boundary_1_2 = (4.5 - np.log(0.75)) / 3.0\n",
    "    \n",
    "    # Plot a vertical red dashed line at that x_1 value\n",
    "    # We get the y-limits from the grid (yy)\n",
    "    plt.vlines(x=x_boundary_1_2, ymin=y_min, ymax=y_max, \n",
    "               colors='red', linestyles='--', linewidth=3, \n",
    "               label=f'Q7A Boundary (x={x_boundary_1_2:.3f})')\n",
    "    \n",
    "    # --- End of Q7A plotting ---\n",
    "    \n",
    "    plt.title(\"Q7B: Optimal Decision Boundaries (Q5 Model)\")\n",
    "    plt.xlabel(\"X1\")\n",
    "    plt.ylabel(\"X2\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# --- è¿è¡Œ Q7B çš„ä»£ç  ---\n",
    "\n",
    "# 1. å®šä¹‰ Q5 çš„å‚æ•°\n",
    "K = 3\n",
    "pis_q5 = np.array([0.3, 0.4, 0.3]);\n",
    "means_q5 = np.array([\n",
    "    [0, 0],\n",
    "    [3, 0],\n",
    "    [0, 3]\n",
    "])\n",
    "covs_q5 = np.array([\n",
    "    [[1, 0], [0, 1]],\n",
    "    [[1, 0], [0, 1]],\n",
    "    [[1, 0], [0, 1]]\n",
    "])\n",
    "\n",
    "# 2. ç”Ÿæˆ Q5 çš„æ•°æ® (æ¥è‡ª Q4/Q5)\n",
    "print(\"Generating Q5 data...\")\n",
    "samples_data_q5, labels_data_q5 = generate_N_GMM_samples(pis_q5, means_q5, covs_q5, K, 1000)\n",
    "print(\"Data generated.\")\n",
    "\n",
    "# 3. è¿è¡Œ Q7B ç»˜å›¾å‡½æ•°\n",
    "plot_decision_boundary(pis_q5, means_q5, covs_q5, K, samples_data_q5, labels_data_q5)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd81c8b7-c58e-40bd-9ad4-82afe63d6884",
   "metadata": {},
   "source": [
    "è¿™æ˜¯ä¸€ä¸ªå¾ˆå¥½çš„é—®é¢˜ï¼Œè¿™ä¸ªé—®é¢˜**æ•…æ„**å¼•å¯¼ä½ ä»â€œè§£æè§£â€ï¼ˆåƒ Q7Aï¼‰è½¬å‘â€œè®¡ç®—è§£â€ï¼ˆåƒ Q7Bï¼‰ã€‚\n",
    "\n",
    "æ­£å¦‚ä½ çš„ç¬”è®° ä¸­æ‰€æç¤ºçš„ï¼š\n",
    "> \"Note: a fully analytical or functional solution might be difficult to obtain; only an interpretable and practical solution for plotting is required.\"\n",
    "\n",
    "**â€œè¿™ä¸ªæ€ä¹ˆåŠï¼Ÿâ€**\n",
    "ç­”æ¡ˆæ˜¯ï¼š**ä½ ä¸éœ€è¦åƒ Q7A é‚£æ ·è§£å‡ºä¸€ä¸ª $x_1 = \\dots$ çš„ç®€å•æ–¹ç¨‹ã€‚**\n",
    "\n",
    "ç›¸åï¼Œä½ åªéœ€è¦ï¼š\n",
    "1.  **å†™ä¸‹å®Œæ•´çš„æ–¹ç¨‹**ï¼ˆå®ƒå°†æ˜¯ä¸€ä¸ªäºŒæ¬¡æ–¹ç¨‹ï¼Œè€Œä¸æ˜¯çº¿æ€§çš„ï¼‰ã€‚\n",
    "2.  **è§£é‡Šä¸ºä»€ä¹ˆå®ƒä¸å†æ˜¯ä¸€æ¡ç›´çº¿**ã€‚\n",
    "\n",
    "---\n",
    "\n",
    "### Q9 çš„â€œå¯è§£é‡Šè§£â€ (Interpretable Solution)\n",
    "\n",
    "**1. ä¸ºä»€ä¹ˆå®ƒä¸ Q7A ä¸åŒï¼Ÿ**\n",
    "\n",
    "* åœ¨ **Q7A** ä¸­ï¼Œåæ–¹å·®çŸ©é˜µæ˜¯**ç›¸ç­‰**çš„ ($\\Sigma_1 = \\Sigma_2 = I$)ã€‚\n",
    "* å½“ä½ å±•å¼€å¯¹æ•°ä¼¼ç„¶æ–¹ç¨‹æ—¶ï¼ŒåŒ…å« $x^T \\Sigma^{-1} x$ çš„**äºŒæ¬¡é¡¹**ï¼ˆå³ $x_1^2, x_2^2$ï¼‰åœ¨ç­‰å¼ä¸¤è¾¹**å®Œå…¨æŠµæ¶ˆ**äº†ã€‚\n",
    "* è¿™ä½¿å¾—æœ€ç»ˆçš„æ–¹ç¨‹æ˜¯ä¸€ä¸ª**çº¿æ€§æ–¹ç¨‹**ï¼ˆä¸€æ¡ç›´çº¿ï¼‰ã€‚\n",
    "\n",
    "* åœ¨ **Q9** ä¸­ï¼Œåæ–¹å·®çŸ©é˜µæ˜¯**ä¸ç­‰**çš„ï¼š\n",
    "    * $\\Sigma_1 = \\left[\\begin{array}{cc} 1 & 0 \\\\ 0 & 1 \\end{array}\\right] \\implies \\Sigma_1^{-1} = \\left[\\begin{array}{cc} 1 & 0 \\\\ 0 & 1 \\end{array}\\right]$\n",
    "    * $\\Sigma_2 = \\left[\\begin{array}{cc} 2 & 0 \\\\ 0 & 2 \\end{array}\\right] \\implies \\Sigma_2^{-1} = \\left[\\begin{array}{cc} 0.5 & 0 \\\\ 0 & 0.5 \\end{array}\\right]$\n",
    "\n",
    "* å› ä¸º $\\Sigma_1^{-1} \\neq \\Sigma_2^{-1}$ï¼Œ**äºŒæ¬¡é¡¹ $x^T \\Sigma^{-1} x$ å°†ä¸ä¼šæŠµæ¶ˆï¼**\n",
    "* è¿™å°†å¯¼è‡´ä¸€ä¸ª**äºŒæ¬¡æ–¹ç¨‹ (Quadratic Equation)**ï¼Œå®ƒçš„è§£æ˜¯ä¸€æ¡**æ›²çº¿**ï¼ˆäºŒæ¬¡æ›²çº¿ï¼Œå¦‚æ¤­åœ†æˆ–åŒæ›²çº¿ï¼‰ï¼Œè€Œä¸å†æ˜¯ç›´çº¿ã€‚\n",
    "\n",
    "**2. Q9 çš„è§£æè§£ï¼ˆæ–¹ç¨‹ï¼‰**\n",
    "\n",
    "å†³ç­–è¾¹ç•Œä»ç„¶ä»è¿™é‡Œå¼€å§‹ï¼š\n",
    "$$P(C=1 | x) = P(C=2 | x)$$\n",
    "\n",
    "å–å¯¹æ•°å¹¶å±•å¼€é«˜æ–¯åˆ†å¸ƒ $\\mathcal{N}(x|\\mu, \\Sigma)$ çš„å®Œæ•´å¯¹æ•°ä¼¼ç„¶å½¢å¼ï¼š\n",
    "$$\\log \\mathcal{N}(x|\\mu, \\Sigma) = -\\frac{1}{2}(x-\\mu)^T \\Sigma^{-1} (x-\\mu) - \\frac{1}{2}\\log(|\\Sigma|) - \\frac{d}{2}\\log(2\\pi)$$\n",
    "\n",
    "æˆ‘ä»¬å°†å¾—åˆ° Q9 çš„**å®Œæ•´è¾¹ç•Œæ–¹ç¨‹**ï¼š\n",
    "$\\log(\\pi_1) - \\frac{1}{2}(x-\\mu_1)^T \\Sigma_1^{-1} (x-\\mu_1) - \\frac{1}{2}\\log(|\\Sigma_1|) = \\log(\\pi_2) - \\frac{1}{2}(x-\\mu_2)^T \\Sigma_2^{-1} (x-\\mu_2) - \\frac{1}{2}\\log(|\\Sigma_2|)$\n",
    "\n",
    "**è¿™å°±æ˜¯ Q9 çš„ç­”æ¡ˆã€‚**\n",
    "\n",
    "è¿™ä¸ªæ–¹ç¨‹å°±æ˜¯ç¬”è®° ä¸­æåˆ°çš„â€œå¯è§£é‡Šè§£â€ã€‚å®ƒæ˜¾ç¤ºäº†æ‰€æœ‰èµ·ä½œç”¨çš„é¡¹ï¼ˆå…ˆéªŒã€å‡å€¼ã€åæ–¹å·®ã€åæ–¹å·®çš„è¡Œåˆ—å¼ï¼‰ã€‚ä½ ä¸éœ€è¦ï¼ˆä¹Ÿä¸åº”è¯¥ï¼‰å°è¯•å»è§£å‡º $x_2$ å…³äº $x_1$ çš„å‡½æ•°ã€‚\n",
    "\n",
    "---\n",
    "\n",
    "### â€œç”¨äºç»˜å›¾çš„å®ç”¨è§£â€ (Practical Solution for Plotting)\n",
    "\n",
    "è¿™ä¸ªæç¤º æ˜¯åœ¨**ç›´æ¥å‘Šè¯‰ä½  Q10 åº”è¯¥æ€ä¹ˆåš**ã€‚\n",
    "\n",
    "* **ä¸è¦**å°è¯•ç”¨ `plt.plot()` æ¥ç”»ä¸Šé¢é‚£ä¸ªå¤æ‚çš„äºŒæ¬¡æ–¹ç¨‹ã€‚\n",
    "* **è¦**ä½¿ç”¨ä½ åœ¨ Q7B ä¸­ä½¿ç”¨çš„**å®Œå…¨ç›¸åŒçš„** `plot_decision_boundary` **ç½‘æ ¼ (Meshgrid) æ–¹æ³•**ã€‚\n",
    "\n",
    "`plot_decision_boundary` å‡½æ•°ï¼ˆä½¿ç”¨ `predict_class_GMM` å’Œ `np.argmax`ï¼‰**æ ¹æœ¬ä¸å…³å¿ƒ**è¾¹ç•Œæ˜¯ç›´çº¿è¿˜æ˜¯æ›²çº¿ã€‚å®ƒåªæ˜¯åœ¨æ¯ä¸ªç‚¹ä¸Šè®¡ç®—â€œè·èƒœè€…â€å¹¶ä¸ºåŒºåŸŸç€è‰²ã€‚\n",
    "\n",
    "**ä½ ä¸º Q10 è¦åšçš„ï¼š**\n",
    "1.  å®šä¹‰ Q9 çš„æ–°å‚æ•°ï¼ˆ`pis_q9`, `means_q9`, `covs_q9`ï¼‰ã€‚\n",
    "2.  è°ƒç”¨ `plot_decision_boundary(pis_q9, means_q9, covs_q9, K, ...)`ã€‚\n",
    "3.  `matplotlib` å°†è‡ªåŠ¨ä¸ºä½ ç»˜åˆ¶å‡ºç”± Q9 é‚£ä¸ªå¤æ‚äºŒæ¬¡æ–¹ç¨‹æ‰€å®šä¹‰çš„**æ›²çº¿è¾¹ç•Œ**ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6343c5d-1973-45a3-bcd4-85d562476159",
   "metadata": {},
   "source": [
    "Here are the solutions to the theoretical questions, based on your provided notes.\n",
    "\n",
    "### **Q1**: Simplify the expression of the EPR(f) to obtain that the EPR is minimized by the conditional mean of Y given X.\n",
    "\n",
    "The goal is to find the function $f(x)$ that minimizes the Expected Prediction Risk (EPR) for the squared loss function.\n",
    "\n",
    "1.  **Start with the definition of EPR:**\n",
    "    $$\\text{EPR}[f] = \\mathbb{E}_{X,Y}[(Y - f(X))^2]$$\n",
    "\n",
    "2.  **Apply the Law of Total Expectation:** We can condition the expectation on $X$. The outer expectation is over $X$, and the inner expectation is over $Y$ given $X$.\n",
    "    $$\\text{EPR}[f] = \\mathbb{E}_X\\left[\\mathbb{E}_{Y|X}[(Y - f(X))^2 | X=x]\\right]$$\n",
    "\n",
    "3.  **Pointwise Minimization:** To minimize the overall EPR, we can find the function $f(x)$ that minimizes the inner expectation for any given point $x$. Let's find the $f(x)$ that minimizes:\n",
    "    $$\\mathbb{E}_{Y|X}[(Y - f(x))^2 | X=x]$$\n",
    "\n",
    "4.  **Expand the Expression:** Let's add and subtract the conditional mean, $g(x) = \\mathbb{E}[Y | X=x]$, inside the square.\n",
    "    $$\\mathbb{E}_{Y|X}[( (Y - g(x)) + (g(x) - f(x)) )^2 | X=x]$$\n",
    "    Expanding this square gives three terms:\n",
    "    $$= \\mathbb{E}[(Y - g(x))^2] + \\mathbb{E}[(g(x) - f(x))^2] + 2\\mathbb{E}[(Y - g(x))(g(x) - f(x))]$$\n",
    "    (Note: All expectations are conditional on $X=x$).\n",
    "\n",
    "5.  **Simplify the Terms:**\n",
    "    * **Term 1:** $\\mathbb{E}[(Y - g(x))^2 | X=x]$. This is the variance of $Y$ given $X$, $\\text{Var}(Y|X=x)$. This term is an inherent property of the data's noise and does not depend on our choice of $f(x)$.\n",
    "    * **Term 2:** $\\mathbb{E}[(g(x) - f(x))^2 | X=x]$. Since $f(x)$ and $g(x)$ are constants with respect to the expectation over $Y$, this simplifies to $(g(x) - f(x))^2$.\n",
    "    * **Term 3 (Cross-term):**\n",
    "        $$2\\mathbb{E}[(Y - g(x))(g(x) - f(x)) | X=x]$$\n",
    "        We can pull the $(g(x) - f(x))$ term out of the expectation:\n",
    "        $$= 2(g(x) - f(x)) \\mathbb{E}[(Y - g(x)) | X=x]$$\n",
    "        By the definition of $g(x)$, we have $\\mathbb{E}[(Y - g(x)) | X=x] = \\mathbb{E}[Y | X=x] - g(x) = g(x) - g(x) = 0$. Therefore, the entire cross-term is zero.\n",
    "\n",
    "6.  **Final Expression:** The conditional risk simplifies to:\n",
    "    $$\\mathbb{E}_{Y|X}[(Y - f(x))^2 | X=x] = \\underbrace{\\mathbb{E}[(Y - g(x))^2 | X=x]}_{\\text{Irreducible Error}} + \\underbrace{(g(x) - f(x))^2}_{\\text{Squared Error}}$$\n",
    "    The first term is the irreducible error (variance) which we cannot change. The second term is a squared value, so it is always $\\ge 0$.\n",
    "\n",
    "7.  **Conclusion:** To minimize this expression, we must choose $f(x)$ to make the second term zero. This is achieved when $f(x) = g(x)$.\n",
    "    Therefore, the optimal $f(x)$ that minimizes the EPR is:\n",
    "    $$f(x) = g(x) = \\mathbb{E}[Y | X=x]$$\n",
    "\n",
    "---\n",
    "\n",
    "### **Q2** Write the expression of the EPR for the classification case simplified as much as possible for general risk functions.\n",
    "\n",
    "1.  **Start with the definition of EPR:** For classification, the model is $\\hat{c}(x)$ and the true class is $C$.\n",
    "    $$\\text{EPR}[\\hat{c}] = \\mathbb{E}_{X,C}[L(C, \\hat{c}(X))]$$\n",
    "   \n",
    "\n",
    "2.  **Apply the Law of Total Expectation:** We condition on $X$.\n",
    "    $$\\text{EPR}[\\hat{c}] = \\mathbb{E}_X\\left[\\mathbb{E}_{C|X}[L(C, \\hat{c}(X)) | X=x]\\right]$$\n",
    "   \n",
    "\n",
    "3.  **Simplify the Inner Expectation:** The inner expectation is over the discrete classes $C \\in \\{1, \\ldots, K\\}$. We can rewrite this expectation as a sum, weighted by the posterior probabilities $P(C=k | X=x)$.\n",
    "    $$\\mathbb{E}_{C|X}[L(C, \\hat{c}(x)) | X=x] = \\sum_{k=1}^K L(C=k, \\hat{c}(x)) P(C=k | X=x)$$\n",
    "\n",
    "4.  **Final Expression:** The outer expectation $\\mathbb{E}_X[\\dots]$ is an integral over all possible values of $x$, weighted by the data's density function $p(x)$. Substituting the inner sum gives the final simplified expression:\n",
    "    $$\\text{EPR}[\\hat{c}] = \\int_x \\left[ \\sum_{k=1}^K L(C=k, \\hat{c}(x)) P(C=k | X=x) \\right] p(x) dx$$\n",
    "\n",
    "---\n",
    "\n",
    "### **Q3** Find the optimal classifier for 0-1 loss: $L(c, \\hat{c}) = \\mathbb{I}(c \\neq \\hat{c})$ known as the Bayes classifier.\n",
    "\n",
    "1.  **Start with the Goal:** From Q2, we know that to minimize the overall EPR, we must find the classifier $\\hat{c}(x)$ that minimizes the conditional risk (the inner expectation) for each $x$.\n",
    "    $$\\hat{c}_{\\text{optimal}}(x) = \\text{argmin}_{\\hat{c}(x)} \\left[ \\sum_{k=1}^K L(C=k, \\hat{c}(x)) P(C=k | X=x) \\right]$$\n",
    "\n",
    "2.  **Insert the 0-1 Loss:** We substitute $L(C=k, \\hat{c}(x)) = \\mathbb{I}(k \\neq \\hat{c}(x))$ into the sum. Let's call our chosen class (our guess) $g = \\hat{c}(x)$.\n",
    "    $$\\text{argmin}_{g} \\left[ \\sum_{k=1}^K \\mathbb{I}(k \\neq g) P(C=k | X=x) \\right]$$\n",
    "\n",
    "3.  **Simplify the Sum:** The term $\\mathbb{I}(k \\neq g)$ is $1$ for every class $k$ that is *not* our guess $g$, and $0$ for the one class $k$ that *is* our guess $g$.\n",
    "    * The term for $k=g$ is $\\mathbb{I}(g \\neq g) P(C=g | X=x) = 0 \\times P(C=g | X=x) = 0$.\n",
    "    * The terms for $k \\neq g$ are $1 \\times P(C=k | X=x)$.\n",
    "    So, the sum simplifies to the total probability of *all classes that are not g*:\n",
    "    $$\\sum_{k \\neq g} P(C=k | X=x)$$\n",
    "\n",
    "4.  **Relate to Maximization:** This sum is the probability of making an incorrect prediction, $P(C \\neq g | X=x)$. Since all probabilities sum to 1, we can rewrite this as:\n",
    "    $$P(C \\neq g | X=x) = 1 - P(C = g | X=x)$$\n",
    "    Our goal is to minimize this expression:\n",
    "    $$\\min_{g} [ 1 - P(C = g | X=x) ]$$\n",
    "\n",
    "5.  **Conclusion (The Bayes Classifier):** Minimizing $1 - P(\\dots)$ is equivalent to **maximizing** $P(\\dots)$.\n",
    "    Therefore, the optimal classifier $\\hat{c}(x)$ is the one that chooses the class $g$ (or $k$) that has the highest posterior probability $P(C=k | X=x)$. This is the **Bayes Classifier**.\n",
    "    $$\\hat{c}_{\\text{Bayes}}(x) = \\text{argmax}_{k \\in \\{1, \\ldots, K\\}} P(C=k | X=x)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04b05ecb-542e-44e8-b085-9dd3bda6b7c3",
   "metadata": {},
   "source": [
    "# System programming for Systems-on-chip\n",
    "\n",
    "```bash\n",
    "# âœ… cd å‘½ä»¤å¸¸ç”¨ç”¨æ³•é€ŸæŸ¥è¡¨\n",
    "\n",
    "# 1. å¸¸ç”¨å¿«æ·æ“ä½œ\n",
    "cd /              # åˆ‡æ¢åˆ°ç³»ç»Ÿæ ¹ç›®å½•\n",
    "cd ~              # åˆ‡æ¢åˆ° home ç›®å½•\n",
    "cd -              # åˆ‡æ¢åˆ°ä¸Šä¸€ä¸ªå·¥ä½œç›®å½•\n",
    "cd ..             # è¿”å›ä¸Šä¸€çº§ç›®å½•\n",
    "cd ../..          # è¿”å›ä¸Šä¸¤çº§ç›®å½•\n",
    "cd /etc           # åˆ‡æ¢åˆ°ç»å¯¹è·¯å¾„ /etc\n",
    "cd Documents      # åˆ‡æ¢åˆ°å½“å‰ç›®å½•ä¸‹çš„ Documents å­ç›®å½•\n",
    "cd ~/Downloads    # åˆ‡æ¢åˆ° home ç›®å½•ä¸‹çš„ Downloads\n",
    "\n",
    "# 2. å¿«æ·ç¬¦å·\n",
    "~   = home ç›®å½•\n",
    ".   = å½“å‰ç›®å½•\n",
    "..  = ä¸Šä¸€çº§ç›®å½•\n",
    "-   = ä¸Šä¸€ä¸ªç›®å½•\n",
    "\n",
    "pwd               # æ˜¾ç¤ºå½“å‰å·¥ä½œç›®å½•\n",
    "\n",
    "```\n",
    "\n",
    "```bash\n",
    "# 1. æ·»åŠ æ–°ç§»å…¥çš„æ–‡ä»¶å¤¹åˆ° Git è·Ÿè¸ª\n",
    "git add .\n",
    "\n",
    "# 2. æ‹‰å–è¿œç¨‹ä»“åº“çš„æœ€æ–°æ›´æ–°ï¼ˆå¹¶å°è¯•è‡ªåŠ¨åˆå¹¶ï¼‰\n",
    "git pull\n",
    "\n",
    "# 3. æäº¤æœ¬åœ°æ›´æ”¹ï¼ˆä½ å¯ä»¥ä¿®æ”¹ commit messageï¼‰\n",
    "git commit -m \"Add new folders\"\n",
    "\n",
    "# 4. æ¨é€åˆ°è¿œç¨‹ä»“åº“\n",
    "git push\n",
    "```\n",
    "\n",
    "```bash\n",
    "# åˆ›å»º Windows ç¬¦å·é“¾æ¥\n",
    "echo 'export MSYS=winsymlinks:nativestrict' >> ~/.bashrc\n",
    "export MSYS=winsymlinks:nativestrict\n",
    "git config --global core.symlinks true\n",
    "\n",
    "# å¼ºåˆ¶åˆ›å»ºç¬¦å·é“¾æ¥\n",
    "git clone -c core.symlinks=true https://github.com/EPFL-LAP/cs473.git\n",
    "git clone -c core.symlinks=true https://github.com/Jerry0209/cs473-SysProgramming.git\n",
    "git clone -c core.symlinks=true git@github.com:Jerry0209/cs473-SysProgramming.git\n",
    "\n",
    "# error: unable to create symlink virtualprototype/programs/helloWorld/support: No such file or directory\n",
    "\n",
    "cd cs473\n",
    "git restore --source=HEAD :/\n",
    "\n",
    "```\n",
    "\n",
    "[Practical work 1 Getting to know the virtual prototype](https://www.notion.so/Practical-work-1-Getting-to-know-the-virtual-prototype-27773d8896f0807bbdddc8eadca28f93?pvs=21)\n",
    "\n",
    "[Practical work 2 The Mandelbrot set](https://www.notion.so/Practical-work-2-The-Mandelbrot-set-28273d8896f08069a5ebf4d85f166878?pvs=21)\n",
    "\n",
    "[Practical work 3 Profiling and memory distance](https://www.notion.so/Practical-work-3-Profiling-and-memory-distance-28573d8896f0807485e5fa029635e4ca?pvs=21)\n",
    "\n",
    "[Practical work 4 Caches](https://www.notion.so/Practical-work-4-Caches-2a073d8896f08015925bfbc2cfc744df?pvs=21)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96dc1a8e-9e6d-4f81-9e41-bfb590ce031b",
   "metadata": {},
   "source": [
    "Here is a breakdown of that concept:\n",
    "\n",
    "1.  **What is the \"correlation in its covariance matrix\"?**\n",
    "    * In your Q9 model, $\\Sigma_3 = \\left[\\begin{array}{cc} 1 & 0.5 \\\\ 0.5 & 1 \\end{array}\\right]$.\n",
    "    * The \"off-diagonal\" terms (the 0.5s) represent the **covariance** between $X_1$ and $X_2$.\n",
    "    * A non-zero covariance (like 0.5) means that $X_1$ and $X_2$ are **correlated**. A positive value means that when $X_1$ is high, $X_2$ tends to be high as well.\n",
    "\n",
    "2.  **What is the \"shape\" of component $c_3$?**\n",
    "    * **No correlation (e.g., $\\Sigma_1 = I$):** The data points form a circular cloud.\n",
    "    * **Correlation (e.g., $\\Sigma_3$):** Because of the positive correlation, the \"cloud\" of data points for $c_3$ is not a circle. It is an **ellipse** that is \"tilted\" or \"stretched out\" along the $y=x$ line (a 45-degree angle).\n",
    "\n",
    "3.  **How does this \"tilt\" the boundary?**\n",
    "    * The decision boundary is the line where the probability of belonging to one component (e.g., $c_1$) is equal to the probability of belonging to another (e.g., $c_3$).\n",
    "    * Because the probability distribution for $c_3$ is *itself* tilted (it's an ellipse at a 45-degree angle), the line of \"equal probability\" between $c_3$ and the circular $c_1$ must also be a curve that \"tilts\" to match $c_3$'s shape.\n",
    "    * The boundary has to \"bend\" to follow the shape of this correlated component. This is why the boundary lines involving $c_3$ are not symmetric with the x/y axes; they are tilted to align with the 45-degree correlation of $\\Sigma_3$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af3bcd9c-3111-4376-87fb-4acca278bf27",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
