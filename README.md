# EE-411: Fundamental of inference and learning 

In this repo you will find the files for the TA sessions and the Homeworks. For all other informations we refer to the Moodle link https://moodle.epfl.ch/course/view.php?id=16783#section-0 and to the catalogue page https://edu.epfl.ch/coursebook/en/fundamentals-of-inference-and-learning-EE-411.


| TP (File) | Key Concepts |
| :--- | :--- |
| **TP 0** | **Python, NumPy, Pandas & Matplotlib Basics** • Basic Python (loops, functions, classes), NumPy (arrays, linear algebra), SciPy (stats), Pandas (data loading), and Matplotlib (plotting). |
| **TP 1** | **Hypothesis Testing & Resampling** • Pandas data handling, Permutation Test, Bootstrap, and Confidence Intervals. |
| **TP 2** | **Optimization, MLE & Bayesian Inference** • Using `scipy.optimize` for minimization. • Maximum Likelihood Estimator (MLE) vs. Median Estimator (MSE). • Cramér-Rao Bound and Jeffreys Prior. |
| **TP 3** | **k-Nearest Neighbors (kNN) from Scratch** • Implementing kNN from scratch for classification. • Calculating distance metrics efficiently (`scipy.spatial.distance.cdist`). • Comparing Training vs. Test Error; analyzing the Bias-Variance Tradeoff (N/k). |
| **TP 4** | **Intro to Scikit-Learn (kNN, Trees, Random Forest)** • Scikit-Learn API (`.fit()`, `.predict()`). • Using `KNeighborsClassifier` and `GridSearchCV` for hyperparameter tuning. • Regression Trees (`DecisionTreeRegressor`) and Random Forest (`RandomForestRegressor`). |
| **TP 5** | **Gradient Descent & Regularization (from scratch)** • Solving Ridge Regression via normal equations. • Implementing Gradient Descent (GD) from scratch. • Implementing GD with Momentum and NAG (Nesterov). • Implementing Sub-gradient Descent and ISTA for LASSO (L1). |
| **TP 6** | **Regularized Linear Regression (Scikit-Learn)** • Using `sklearn.linear_model` for OLS, Ridge (L2), and Lasso (L1). • Using `make_pipeline` with `StandardScaler`. • Plotting the Regularization Path and observing Lasso's sparsity. |
| **TP 7** | **Logistic Regression & Support Vector Machines (SVM)** • Implementing Logistic Regression (Sigmoid, Log-Loss, GD) from scratch. • Using `sklearn.linear_model.LogisticRegression` (SoftMax) for multi-class classification (MNIST). • Using `sklearn.svm.SVC` (linear kernel) with Cross-Validation. |
| **TP 8** | **Kernel Methods & Backpropagation** • Implementing Kernel Ridge Regression (KRR) from scratch using polynomial and RBF feature maps. • Using `sklearn.svm.SVC` with different kernels (`poly`, `rbf`, `sigmoid`) on MNIST. • Implementing Backpropagation (forward and backward pass) for a simple MLP from scratch. |
| **TP 9** | **Intro to PyTorch & Neural Networks** • PyTorch basics: Tensors, Autograd (automatic differentiation). • Building networks with `torch.nn.Module`, `nn.Linear`, `nn.Sequential`. • Training on MNIST: Using `DataLoader`, Optimizers (SGD, Adam), and Loss Functions (Cross-Entropy). • Building a simple MLP and CNN (Convolutional Neural Network). |


